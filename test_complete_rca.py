"""
å®Œæ•´çš„RCAæµç¨‹æµ‹è¯•
æµ‹è¯•ä»ç”¨æˆ·è¾“å…¥åˆ°æœ€ç»ˆRCAç»“è®ºçš„å®Œæ•´é“¾è·¯
"""

import asyncio
import sys
import os
import json
import logging
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'src'))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def test_rag_integration():
    """æµ‹è¯•RAGé›†æˆæ˜¯å¦å¯ç”¨"""
    print("ğŸ” æµ‹è¯•RAGé›†æˆ...")
    try:
        import weaviate
        from sentence_transformers import SentenceTransformer
        from neo4j import GraphDatabase
        
        # æµ‹è¯•Weaviate
        client = weaviate.Client(url="http://localhost:8080")
        embedding_count = client.query.aggregate("EmbeddingCollection").with_meta_count().do()
        fulltext_count = client.query.aggregate("FullTextCollection").with_meta_count().do()
        
        embedding_records = embedding_count['data']['Aggregate']['EmbeddingCollection'][0]['meta']['count']
        fulltext_records = fulltext_count['data']['Aggregate']['FullTextCollection'][0]['meta']['count']
        
        print(f"âœ… Weaviate: {embedding_records} embedding records, {fulltext_records} fulltext records")
        
        # æµ‹è¯•Neo4j
        driver = GraphDatabase.driver("bolt://localhost:7687", auth=("neo4j", "aiops123"))
        with driver.session() as session:
            node_count = session.run("MATCH (n) RETURN count(n) as count").single()["count"]
            rel_count = session.run("MATCH ()-[r]->() RETURN count(r) as count").single()["count"]
        driver.close()
        
        print(f"âœ… Neo4j: {node_count} nodes, {rel_count} relationships")
        
        return embedding_records > 0 and node_count > 0
        
    except Exception as e:
        print(f"âŒ RAGé›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False


async def simulate_rag_search(incident_description):
    """æ¨¡æ‹ŸRAGæœç´¢è¿‡ç¨‹"""
    print(f"ğŸ” æ¨¡æ‹ŸRAGæœç´¢: '{incident_description}'")
    
    try:
        import weaviate
        from sentence_transformers import SentenceTransformer
        from neo4j import GraphDatabase
        
        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        client = weaviate.Client(url="http://localhost:8080")
        
        # 1. å‘é‡æœç´¢ç›¸å…³æ—¥å¿—
        query_vector = model.encode(incident_description).tolist()
        
        vector_result = (
            client.query
            .get("EmbeddingCollection", ["content", "service_name", "log_file", "timestamp"])
            .with_near_vector({"vector": query_vector})
            .with_limit(5)
            .with_additional(["certainty"])
            .do()
        )
        
        log_evidence = vector_result["data"]["Get"]["EmbeddingCollection"]
        print(f"   ğŸ“‹ æ‰¾åˆ° {len(log_evidence)} æ¡ç›¸å…³æ—¥å¿—")
        
        for i, log in enumerate(log_evidence[:3]):
            certainty = log["_additional"]["certainty"]
            content = log["content"][:60]
            service = log.get("service_name", "unknown")
            print(f"      {i+1}. [{service}] {content}... (certainty: {certainty:.3f})")
        
        # 2. å…¨æ–‡æœç´¢çŸ¥è¯†æ–‡æ¡£
        bm25_result = (
            client.query
            .get("FullTextCollection", ["content", "source_type", "service_name"])
            .with_bm25(query=incident_description)
            .with_limit(3)
            .do()
        )
        
        knowledge_evidence = bm25_result["data"]["Get"]["FullTextCollection"]
        print(f"   ğŸ“š æ‰¾åˆ° {len(knowledge_evidence)} æ¡ç›¸å…³çŸ¥è¯†")
        
        for i, doc in enumerate(knowledge_evidence):
            content = doc["content"][:60]
            source = doc.get("source_type", "unknown")
            print(f"      {i+1}. [{source}] {content}...")
        
        # 3. æŸ¥è¯¢çŸ¥è¯†å›¾è°±
        # ä»incident_descriptionä¸­æå–æœåŠ¡å
        services_mentioned = []
        for service in ["service-a", "service-b", "service-c", "database", "redis"]:
            if service in incident_description.lower():
                services_mentioned.append(service)
        
        graph_evidence = []
        if services_mentioned:
            driver = GraphDatabase.driver("bolt://localhost:7687", auth=("neo4j", "aiops123"))
            
            with driver.session() as session:
                for service in services_mentioned:
                    result = session.run(
                        """
                        MATCH (s:Service {name: $service})-[r]-(related)
                        RETURN related.name as name, labels(related) as labels, type(r) as relation
                        LIMIT 5
                        """,
                        service=service
                    )
                    
                    for record in result:
                        graph_evidence.append({
                            'service': service,
                            'related': record['name'],
                            'labels': record['labels'],
                            'relation': record['relation']
                        })
            
            driver.close()
        
        print(f"   ğŸ•¸ï¸ æ‰¾åˆ° {len(graph_evidence)} æ¡å›¾è°±å…³ç³»")
        for i, rel in enumerate(graph_evidence[:3]):
            print(f"      {i+1}. {rel['service']} --{rel['relation']}--> {rel['related']} {rel['labels']}")
        
        return {
            "log_evidence": log_evidence,
            "knowledge_evidence": knowledge_evidence,
            "graph_evidence": graph_evidence,
            "total_evidence": len(log_evidence) + len(knowledge_evidence) + len(graph_evidence)
        }
        
    except Exception as e:
        print(f"âŒ RAGæœç´¢æ¨¡æ‹Ÿå¤±è´¥: {e}")
        return None


async def simulate_rca_reasoning(evidence_data, incident_description):
    """æ¨¡æ‹ŸRCAæ¨ç†è¿‡ç¨‹"""
    print("\nğŸ§  æ¨¡æ‹ŸRCAæ¨ç†è¿‡ç¨‹...")
    
    if not evidence_data:
        print("âŒ ç¼ºå°‘è¯æ®æ•°æ®ï¼Œæ— æ³•è¿›è¡Œæ¨ç†")
        return None
    
    try:
        # 1. ç—‡çŠ¶åˆ†æ
        symptoms = []
        incident_lower = incident_description.lower()
        
        if "cpu" in incident_lower:
            symptoms.append({"type": "performance", "symptom": "CPUé«˜ä½¿ç”¨ç‡", "severity": "high"})
        if "timeout" in incident_lower or "è¶…æ—¶" in incident_lower:
            symptoms.append({"type": "performance", "symptom": "å“åº”è¶…æ—¶", "severity": "high"})
        if "error" in incident_lower or "é”™è¯¯" in incident_lower:
            symptoms.append({"type": "functional", "symptom": "åŠŸèƒ½é”™è¯¯", "severity": "medium"})
        if "slow" in incident_lower or "æ…¢" in incident_lower:
            symptoms.append({"type": "performance", "symptom": "å“åº”ç¼“æ…¢", "severity": "medium"})
        
        print(f"   ğŸ“Š è¯†åˆ«ç—‡çŠ¶: {len(symptoms)} ä¸ª")
        for symptom in symptoms:
            print(f"      - {symptom['symptom']} ({symptom['type']}, {symptom['severity']})")
        
        # 2. æ ¹å› æ¨ç†
        potential_causes = []
        
        # åŸºäºæ—¥å¿—è¯æ®æ¨ç†
        for log in evidence_data.get("log_evidence", [])[:3]:
            content = log["content"].lower()
            if "cpu" in content and "high" in content:
                potential_causes.append({
                    "cause": "CPUèµ„æºä¸è¶³",
                    "confidence": log["_additional"]["certainty"],
                    "evidence_type": "log",
                    "source": log.get("service_name", "unknown")
                })
            elif "memory" in content:
                potential_causes.append({
                    "cause": "å†…å­˜èµ„æºé—®é¢˜",
                    "confidence": log["_additional"]["certainty"],
                    "evidence_type": "log",
                    "source": log.get("service_name", "unknown")
                })
        
        # åŸºäºå›¾è°±è¯æ®æ¨ç†
        for rel in evidence_data.get("graph_evidence", [])[:3]:
            if rel['relation'] == 'DEPENDS_ON':
                potential_causes.append({
                    "cause": f"{rel['service']}ä¾èµ–çš„{rel['related']}æœåŠ¡é—®é¢˜",
                    "confidence": 0.7,
                    "evidence_type": "dependency",
                    "source": rel['service']
                })
            elif rel['relation'] == 'DEPLOYED_ON':
                potential_causes.append({
                    "cause": f"{rel['related']}ä¸»æœºèµ„æºé—®é¢˜",
                    "confidence": 0.6,
                    "evidence_type": "deployment",
                    "source": rel['service']
                })
        
        # æ ¹æ®ç½®ä¿¡åº¦æ’åº
        potential_causes.sort(key=lambda x: x["confidence"], reverse=True)
        
        print(f"   ğŸ¯ æ½œåœ¨æ ¹å› : {len(potential_causes)} ä¸ª")
        for i, cause in enumerate(potential_causes[:3]):
            print(f"      {i+1}. {cause['cause']} (ç½®ä¿¡åº¦: {cause['confidence']:.3f}, æ¥æº: {cause['evidence_type']})")
        
        # 3. è§£å†³æ–¹æ¡ˆå»ºè®®
        solutions = []
        
        for cause in potential_causes[:2]:  # å–å‰2ä¸ªæœ€å¯èƒ½çš„æ ¹å› 
            if "CPU" in cause["cause"]:
                solutions.append({
                    "solution": "æ‰©å®¹CPUèµ„æºæˆ–ä¼˜åŒ–CPUå¯†é›†å‹æ“ä½œ",
                    "priority": "high",
                    "estimated_time": "30åˆ†é’Ÿ"
                })
            elif "å†…å­˜" in cause["cause"]:
                solutions.append({
                    "solution": "æ£€æŸ¥å†…å­˜æ³„éœ²ï¼Œé‡å¯ç›¸å…³æœåŠ¡",
                    "priority": "high", 
                    "estimated_time": "15åˆ†é’Ÿ"
                })
            elif "ä¾èµ–" in cause["cause"]:
                solutions.append({
                    "solution": "æ£€æŸ¥ä¾èµ–æœåŠ¡çŠ¶æ€ï¼Œä¿®å¤æœåŠ¡é—´é€šä¿¡",
                    "priority": "medium",
                    "estimated_time": "45åˆ†é’Ÿ"
                })
            elif "ä¸»æœº" in cause["cause"]:
                solutions.append({
                    "solution": "æ£€æŸ¥ä¸»æœºèµ„æºä½¿ç”¨æƒ…å†µï¼Œè€ƒè™‘è¿ç§»æœåŠ¡",
                    "priority": "medium",
                    "estimated_time": "60åˆ†é’Ÿ"
                })
        
        print(f"   ğŸ’¡ è§£å†³æ–¹æ¡ˆ: {len(solutions)} ä¸ª")
        for i, solution in enumerate(solutions):
            print(f"      {i+1}. {solution['solution']} ({solution['priority']}, é¢„è®¡{solution['estimated_time']})")
        
        # 4. æœ€ç»ˆRCAç»“è®º
        if potential_causes:
            primary_cause = potential_causes[0]
            
            rca_conclusion = {
                "incident_description": incident_description,
                "primary_root_cause": primary_cause["cause"],
                "confidence": primary_cause["confidence"],
                "symptoms_count": len(symptoms),
                "evidence_count": evidence_data["total_evidence"],
                "alternative_causes": [c["cause"] for c in potential_causes[1:3]],
                "recommended_solutions": solutions[:2],
                "analysis_timestamp": datetime.now().isoformat()
            }
            
            print(f"\nâœ… RCAåˆ†æå®Œæˆ:")
            print(f"   ä¸»è¦æ ¹å› : {rca_conclusion['primary_root_cause']}")
            print(f"   ç½®ä¿¡åº¦: {rca_conclusion['confidence']:.3f}")
            print(f"   è¯æ®æ€»æ•°: {rca_conclusion['evidence_count']}")
            print(f"   æ¨èæ–¹æ¡ˆ: {len(rca_conclusion['recommended_solutions'])} ä¸ª")
            
            return rca_conclusion
        else:
            print("âŒ æ— æ³•ç¡®å®šæ ¹æœ¬åŸå› ")
            return None
        
    except Exception as e:
        print(f"âŒ RCAæ¨ç†å¤±è´¥: {e}")
        return None


async def test_complete_rca_scenarios():
    """æµ‹è¯•å®Œæ•´RCAåœºæ™¯"""
    print("\nğŸš€ å¼€å§‹å®Œæ•´RCAåœºæ™¯æµ‹è¯•")
    print("=" * 60)
    
    # å‡†å¤‡æµ‹è¯•åœºæ™¯
    test_scenarios = [
        {
            "title": "Incident 001 - Service-B CPUè¿‡è½½",
            "description": "service-b CPUä½¿ç”¨ç‡è¿‡é«˜å¯¼è‡´å“åº”è¶…æ—¶ï¼Œç”¨æˆ·åé¦ˆé¡µé¢åŠ è½½ç¼“æ…¢",
            "expected_keywords": ["CPU", "service-b", "timeout", "performance"]
        },
        {
            "title": "Incident 002 - æ•°æ®åº“è¿æ¥é—®é¢˜", 
            "description": "databaseæœåŠ¡è¿æ¥å¤±è´¥ï¼Œå¤šä¸ªæœåŠ¡æ— æ³•è®¿é—®æ•°æ®åº“",
            "expected_keywords": ["database", "connection", "dependency", "error"]
        },
        {
            "title": "Incident 003 - ç£ç›˜IOç“¶é¢ˆ",
            "description": "d1ä¸»æœºç£ç›˜IOè¿‡é«˜ï¼Œéƒ¨ç½²åœ¨å…¶ä¸Šçš„æœåŠ¡å“åº”å˜æ…¢",
            "expected_keywords": ["disk", "IO", "d1", "performance"]
        }
    ]
    
    successful_scenarios = 0
    
    for i, scenario in enumerate(test_scenarios, 1):
        print(f"\nğŸ“‹ åœºæ™¯ {i}: {scenario['title']}")
        print(f"   æè¿°: {scenario['description']}")
        
        # 1. RAGæœç´¢
        evidence_data = await simulate_rag_search(scenario['description'])
        
        if evidence_data and evidence_data["total_evidence"] >= 3:
            print(f"   âœ… RAGæœç´¢æˆåŠŸ - æ‰¾åˆ° {evidence_data['total_evidence']} æ¡è¯æ®")
            
            # 2. RCAæ¨ç†
            rca_result = await simulate_rca_reasoning(evidence_data, scenario['description'])
            
            if rca_result:
                print(f"   âœ… RCAæ¨ç†æˆåŠŸ")
                print(f"      æ ¹å› : {rca_result['primary_root_cause']}")
                print(f"      ç½®ä¿¡åº¦: {rca_result['confidence']:.3f}")
                print(f"      è§£å†³æ–¹æ¡ˆ: {len(rca_result['recommended_solutions'])} ä¸ª")
                successful_scenarios += 1
            else:
                print(f"   âŒ RCAæ¨ç†å¤±è´¥")
        else:
            print(f"   âŒ RAGæœç´¢å¤±è´¥ - è¯æ®ä¸è¶³")
    
    print(f"\n" + "=" * 60)
    print(f"ğŸ“Š RCAåœºæ™¯æµ‹è¯•ç»“æœ: {successful_scenarios}/{len(test_scenarios)} æˆåŠŸ")
    
    if successful_scenarios == len(test_scenarios):
        print("ğŸ‰ æ‰€æœ‰RCAåœºæ™¯æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ’¡ Agentå…·å¤‡å®Œæ•´çš„RCAåˆ†æèƒ½åŠ›ï¼Œå¯ä»¥ï¼š")
        print("   - ä»ç”¨æˆ·æè¿°ä¸­ç†è§£é—®é¢˜ç—‡çŠ¶")
        print("   - é€šè¿‡RAGæœç´¢è·å–ç›¸å…³è¯æ®") 
        print("   - åŸºäºè¯æ®è¿›è¡Œæ ¹å› æ¨ç†")
        print("   - æä¾›å…·ä½“çš„è§£å†³æ–¹æ¡ˆå»ºè®®")
        print("   - ç»™å‡ºç½®ä¿¡åº¦è¯„ä¼°")
    elif successful_scenarios >= len(test_scenarios) * 0.7:
        print("âœ… å¤§éƒ¨åˆ†RCAåœºæ™¯æˆåŠŸ")
        print("ğŸ’¡ AgentåŸºæœ¬å…·å¤‡RCAåˆ†æèƒ½åŠ›ï¼Œå»ºè®®ä¼˜åŒ–å¤±è´¥åœºæ™¯")
    else:
        print("âš ï¸ RCAåœºæ™¯æµ‹è¯•å­˜åœ¨é—®é¢˜") 
        print("ğŸ’¡ å»ºè®®æ£€æŸ¥RAGæ•°æ®å®Œæ•´æ€§å’Œæ¨ç†é€»è¾‘")
    
    return successful_scenarios == len(test_scenarios)


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å®Œæ•´RCAæµç¨‹æµ‹è¯•")
    print("=" * 60)
    
    # 1. æµ‹è¯•RAGé›†æˆ
    rag_ready = await test_rag_integration()
    
    if not rag_ready:
        print("âŒ RAGé›†æˆæœªå°±ç»ªï¼Œæ— æ³•è¿›è¡ŒRCAæµ‹è¯•")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ python run_pipelines.py å»ºç«‹æ•°æ®ç´¢å¼•")
        return
    
    # 2. æµ‹è¯•å®Œæ•´RCAåœºæ™¯
    rca_success = await test_complete_rca_scenarios()
    
    print(f"\n" + "=" * 60)
    print("ğŸ“‹ å®Œæ•´RCAæµç¨‹æµ‹è¯•æ€»ç»“:")
    print(f"   RAGé›†æˆ: {'âœ… å°±ç»ª' if rag_ready else 'âŒ æœªå°±ç»ª'}")
    print(f"   RCAåœºæ™¯: {'âœ… é€šè¿‡' if rca_success else 'âŒ å¤±è´¥'}")
    
    if rag_ready and rca_success:
        print("\nğŸ‰ æ­å–œï¼RCA Pipelineå®Œå…¨å°±ç»ª")
        print("ğŸ’¡ ç°åœ¨å¯ä»¥åœ¨web UIä¸­æµ‹è¯•:")
        print("   1. å¯åŠ¨APIæœåŠ¡: uvicorn src.api.main:app --reload")
        print("   2. è®¿é—® http://localhost:8000/docs")
        print("   3. ä½¿ç”¨/chatç«¯ç‚¹æµ‹è¯•RCAæŸ¥è¯¢")
        print("   4. ç¤ºä¾‹æŸ¥è¯¢: 'service-b CPUä½¿ç”¨ç‡è¿‡é«˜ï¼Œå“åº”è¶…æ—¶'")


if __name__ == "__main__":
    asyncio.run(main())