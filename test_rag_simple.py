"""
ç®€åŒ–çš„RAG Pipelineæµ‹è¯•
ç›´æ¥æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½ï¼Œé¿å…å¤æ‚çš„å¯¼å…¥ä¾èµ–
"""

import asyncio
import sys
import os
import json
import logging
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'src'))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def test_weaviate_connection():
    """æµ‹è¯•Weaviateè¿æ¥"""
    print("\n=== æµ‹è¯•Weaviateè¿æ¥ ===")
    try:
        import weaviate
        
        client = weaviate.Client(
            url="http://localhost:8080",
            timeout_config=(5, 60)
        )
        
        # æ£€æŸ¥è¿æ¥
        result = client.cluster.get_nodes_status()
        print("âœ… Weaviateè¿æ¥æˆåŠŸ")
        print(f"   èŠ‚ç‚¹çŠ¶æ€: {len(result)} ä¸ªèŠ‚ç‚¹")
        
        # æ£€æŸ¥ç°æœ‰schema
        schema = client.schema.get()
        class_names = [cls['class'] for cls in schema.get('classes', [])]
        print(f"   ç°æœ‰Collections: {class_names}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Weaviateè¿æ¥å¤±è´¥: {e}")
        return False


async def test_embedding_service():
    """æµ‹è¯•åµŒå…¥æœåŠ¡"""
    print("\n=== æµ‹è¯•åµŒå…¥æœåŠ¡ ===")
    try:
        from sentence_transformers import SentenceTransformer
        
        # åŠ è½½æ¨¡å‹
        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        
        # æµ‹è¯•ç¼–ç 
        test_texts = [
            "service-b CPUä½¿ç”¨ç‡è¿‡é«˜",
            "æ•°æ®åº“è¿æ¥è¶…æ—¶é—®é¢˜", 
            "Kubernetes Podé‡å¯å¾ªç¯"
        ]
        
        embeddings = model.encode(test_texts)
        print(f"âœ… åµŒå…¥æœåŠ¡æ­£å¸¸")
        print(f"   å‘é‡ç»´åº¦: {embeddings.shape[1]}")
        print(f"   æµ‹è¯•æ–‡æœ¬æ•°: {embeddings.shape[0]}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åµŒå…¥æœåŠ¡å¤±è´¥: {e}")
        return False


async def test_neo4j_connection():
    """æµ‹è¯•Neo4jè¿æ¥"""
    print("\n=== æµ‹è¯•Neo4jè¿æ¥ ===")
    try:
        from neo4j import GraphDatabase
        
        driver = GraphDatabase.driver(
            "bolt://localhost:7687",
            auth=("neo4j", "aiops123")
        )
        
        # æµ‹è¯•è¿æ¥
        with driver.session() as session:
            result = session.run("RETURN 1 as test")
            record = result.single()
            assert record["test"] == 1
        
        print("âœ… Neo4jè¿æ¥æˆåŠŸ")
        
        # æ£€æŸ¥ç°æœ‰æ•°æ®
        with driver.session() as session:
            node_count_result = session.run("MATCH (n) RETURN count(n) as count")
            node_count = node_count_result.single()["count"]
            
            rel_count_result = session.run("MATCH ()-[r]->() RETURN count(r) as count")
            rel_count = rel_count_result.single()["count"]
        
        print(f"   ç°æœ‰èŠ‚ç‚¹: {node_count} ä¸ª")
        print(f"   ç°æœ‰å…³ç³»: {rel_count} ä¸ª")
        
        driver.close()
        return True
        
    except Exception as e:
        print(f"âŒ Neo4jè¿æ¥å¤±è´¥: {e}")
        return False


async def test_data_availability():
    """æµ‹è¯•æ•°æ®å¯ç”¨æ€§"""
    print("\n=== æµ‹è¯•æ•°æ®å¯ç”¨æ€§ ===")
    try:
        data_stats = {}
        
        # æ£€æŸ¥å„æ•°æ®ç›®å½•
        data_dirs = {
            'logs': './data/logs/',
            'wiki': './data/wiki/',
            'gitlab': './data/gitlab/',
            'jira': './data/jira/'
        }
        
        for name, path in data_dirs.items():
            dir_path = Path(path)
            if dir_path.exists():
                files = list(dir_path.glob("*.*"))
                data_stats[name] = len(files)
                print(f"âœ… {name}: {len(files)} ä¸ªæ–‡ä»¶")
                
                # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
                for file in files[:3]:
                    print(f"   - {file.name}")
                if len(files) > 3:
                    print(f"   - ... è¿˜æœ‰ {len(files) - 3} ä¸ªæ–‡ä»¶")
            else:
                data_stats[name] = 0
                print(f"âŒ {name}: ç›®å½•ä¸å­˜åœ¨")
        
        total_files = sum(data_stats.values())
        print(f"\nğŸ“Š æ€»æ•°æ®æ–‡ä»¶: {total_files} ä¸ª")
        
        return total_files > 0
        
    except Exception as e:
        print(f"âŒ æ•°æ®å¯ç”¨æ€§æ£€æŸ¥å¤±è´¥: {e}")
        return False


async def test_basic_rag_workflow():
    """æµ‹è¯•åŸºæœ¬RAGå·¥ä½œæµ"""
    print("\n=== æµ‹è¯•åŸºæœ¬RAGå·¥ä½œæµ ===")
    try:
        import weaviate
        from sentence_transformers import SentenceTransformer
        
        # 1. è¿æ¥Weaviate
        client = weaviate.Client(url="http://localhost:8080")
        
        # 2. åˆ›å»ºç®€åŒ–schema
        try:
            client.schema.delete_class("TestCollection")
        except:
            pass
        
        test_schema = {
            "class": "TestCollection",
            "description": "æµ‹è¯•Collection",
            "vectorizer": "none",
            "properties": [
                {
                    "name": "content",
                    "dataType": ["text"],
                    "description": "å†…å®¹"
                },
                {
                    "name": "source_type",
                    "dataType": ["string"],
                    "description": "æ•°æ®æºç±»å‹"
                }
            ]
        }
        
        client.schema.create_class(test_schema)
        print("âœ… æµ‹è¯•schemaåˆ›å»ºæˆåŠŸ")
        
        # 3. æ·»åŠ æµ‹è¯•æ•°æ®
        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        
        test_docs = [
            "service-b CPUä½¿ç”¨ç‡è¾¾åˆ°95%ï¼Œè¯·æ±‚å“åº”è¶…æ—¶",
            "æ•°æ®åº“è¿æ¥æ± è€—å°½ï¼ŒMySQLå“åº”ç¼“æ…¢",
            "Kubernetes Podä¸æ–­é‡å¯ï¼Œé•œåƒæ‹‰å–å¤±è´¥",
            "Rediså†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œç¼“å­˜å‘½ä¸­ç‡ä¸‹é™"
        ]
        
        for i, doc in enumerate(test_docs):
            vector = model.encode(doc).tolist()
            
            client.data_object.create(
                data_object={
                    "content": doc,
                    "source_type": "logs"
                },
                class_name="TestCollection",
                vector=vector
            )
        
        print(f"âœ… æ·»åŠ æµ‹è¯•æ•°æ®: {len(test_docs)} æ¡")
        
        # 4. æµ‹è¯•æœç´¢
        query = "CPUæ€§èƒ½é—®é¢˜"
        query_vector = model.encode(query).tolist()
        
        # å‘é‡æœç´¢
        vector_result = (
            client.query
            .get("TestCollection", ["content", "source_type"])
            .with_near_vector({"vector": query_vector, "certainty": 0.5})
            .with_limit(3)
            .with_additional(["certainty"])
            .do()
        )
        
        vector_results = vector_result["data"]["Get"]["TestCollection"]
        print(f"âœ… å‘é‡æœç´¢: æ‰¾åˆ° {len(vector_results)} æ¡ç»“æœ")
        
        if vector_results:
            top_result = vector_results[0]
            certainty = top_result["_additional"]["certainty"]
            content = top_result["content"][:50]
            print(f"   æœ€ä½³åŒ¹é…: {certainty:.3f} - {content}...")
        
        # BM25æœç´¢
        bm25_result = (
            client.query
            .get("TestCollection", ["content", "source_type"])
            .with_bm25(query=query)
            .with_limit(3)
            .with_additional(["score"])
            .do()
        )
        
        bm25_results = bm25_result["data"]["Get"]["TestCollection"]
        print(f"âœ… BM25æœç´¢: æ‰¾åˆ° {len(bm25_results)} æ¡ç»“æœ")
        
        # æ··åˆæœç´¢
        hybrid_result = (
            client.query
            .get("TestCollection", ["content", "source_type"])
            .with_hybrid(query=query, vector=query_vector, alpha=0.7)
            .with_limit(3)
            .with_additional(["score"])
            .do()
        )
        
        hybrid_results = hybrid_result["data"]["Get"]["TestCollection"]
        print(f"âœ… æ··åˆæœç´¢: æ‰¾åˆ° {len(hybrid_results)} æ¡ç»“æœ")
        
        # æ¸…ç†æµ‹è¯•æ•°æ®
        client.schema.delete_class("TestCollection")
        print("âœ… æ¸…ç†æµ‹è¯•æ•°æ®å®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŸºæœ¬RAGå·¥ä½œæµæµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_log_parsing():
    """æµ‹è¯•æ—¥å¿—è§£æåŠŸèƒ½"""
    print("\n=== æµ‹è¯•æ—¥å¿—è§£æ ===")
    try:
        # è¯»å–ç¤ºä¾‹æ—¥å¿—
        log_file = Path("./data/logs/incident_001_service_b_cpu_overload.log")
        if not log_file.exists():
            print("âŒ ç¤ºä¾‹æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        with open(log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()[:10]  # åªè¯»å‰10è¡Œè¿›è¡Œæµ‹è¯•
        
        print(f"ğŸ“„ è¯»å–æ—¥å¿—æ–‡ä»¶: {log_file.name}")
        print(f"   æ€»è¡Œæ•°: {len(lines)}")
        
        # ç®€å•çš„æ—¥å¿—è§£æ
        import re
        
        log_pattern = re.compile(
            r'(?P<timestamp>\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d{3}Z)\s+'
            r'\[(?P<level>\w+)\]\s+'
            r'(?P<service>[\w-]+):\s+'
            r'(?P<message>.*)'
        )
        
        parsed_count = 0
        for i, line in enumerate(lines):
            if line.strip() and not line.startswith('#'):
                match = log_pattern.match(line.strip())
                if match:
                    parsed_count += 1
                    if i < 3:  # æ˜¾ç¤ºå‰3ä¸ªè§£æç»“æœ
                        groups = match.groupdict()
                        print(f"   è§£æè¡Œ {i+1}: {groups['service']} [{groups['level']}] {groups['message'][:40]}...")
        
        print(f"âœ… æˆåŠŸè§£æ: {parsed_count}/{len([l for l in lines if l.strip() and not l.startswith('#')])} è¡Œ")
        
        return parsed_count > 0
        
    except Exception as e:
        print(f"âŒ æ—¥å¿—è§£ææµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_knowledge_data():
    """æµ‹è¯•çŸ¥è¯†æ•°æ®"""
    print("\n=== æµ‹è¯•çŸ¥è¯†æ•°æ® ===")
    try:
        # æµ‹è¯•Wikiæ•°æ®
        wiki_file = Path("./data/wiki/sample_wiki.json")
        if wiki_file.exists():
            with open(wiki_file, 'r', encoding='utf-8') as f:
                wiki_data = json.load(f)
            
            print(f"âœ… Wikiæ•°æ®: {len(wiki_data)} ä¸ªæ–‡æ¡£")
            for doc in wiki_data[:2]:
                print(f"   - {doc.get('title', '')[:50]}...")
        
        # æµ‹è¯•GitLabæ•°æ®
        gitlab_file = Path("./data/gitlab/sample_gitlab.json")
        if gitlab_file.exists():
            with open(gitlab_file, 'r', encoding='utf-8') as f:
                gitlab_data = json.load(f)
            
            print(f"âœ… GitLabæ•°æ®: {len(gitlab_data)} ä¸ªé¡¹ç›®")
        
        # æµ‹è¯•Jiraæ•°æ®
        jira_file = Path("./data/jira/sample_jira.json")
        if jira_file.exists():
            with open(jira_file, 'r', encoding='utf-8') as f:
                jira_data = json.load(f)
            
            print(f"âœ… Jiraæ•°æ®: {len(jira_data)} ä¸ªå·¥å•")
        
        return True
        
    except Exception as e:
        print(f"âŒ çŸ¥è¯†æ•°æ®æµ‹è¯•å¤±è´¥: {e}")
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹RAG Pipelineç®€åŒ–æµ‹è¯•")
    print("=" * 50)
    
    test_results = []
    
    # 1. æµ‹è¯•æ•°æ®å¯ç”¨æ€§
    data_ok = await test_data_availability()
    test_results.append(("æ•°æ®å¯ç”¨æ€§", data_ok))
    
    # 2. æµ‹è¯•Weaviateè¿æ¥
    weaviate_ok = await test_weaviate_connection()
    test_results.append(("Weaviateè¿æ¥", weaviate_ok))
    
    # 3. æµ‹è¯•Neo4jè¿æ¥  
    neo4j_ok = await test_neo4j_connection()
    test_results.append(("Neo4jè¿æ¥", neo4j_ok))
    
    # 4. æµ‹è¯•åµŒå…¥æœåŠ¡
    embedding_ok = await test_embedding_service()
    test_results.append(("åµŒå…¥æœåŠ¡", embedding_ok))
    
    # 5. æµ‹è¯•æ—¥å¿—è§£æ
    log_parsing_ok = await test_log_parsing()
    test_results.append(("æ—¥å¿—è§£æ", log_parsing_ok))
    
    # 6. æµ‹è¯•çŸ¥è¯†æ•°æ®
    knowledge_ok = await test_knowledge_data()
    test_results.append(("çŸ¥è¯†æ•°æ®", knowledge_ok))
    
    # 7. æµ‹è¯•åŸºæœ¬RAGå·¥ä½œæµ
    if weaviate_ok and embedding_ok:
        rag_workflow_ok = await test_basic_rag_workflow()
        test_results.append(("RAGå·¥ä½œæµ", rag_workflow_ok))
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 50)
    print("ğŸ“‹ æµ‹è¯•ç»“æœæ€»ç»“:")
    print("=" * 50)
    
    for test_name, result in test_results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name:<15} {status}")
    
    passed_tests = sum(1 for _, result in test_results if result)
    total_tests = len(test_results)
    
    print(f"\nğŸ“Š æ€»ä½“ç»“æœ: {passed_tests}/{total_tests} æµ‹è¯•é€šè¿‡")
    
    # ç»™å‡ºå»ºè®®
    if passed_tests == total_tests:
        print("ğŸ‰ æ‰€æœ‰åŸºç¡€ç»„ä»¶å·¥ä½œæ­£å¸¸ï¼")
        print("ğŸ’¡ å»ºè®®: ç°åœ¨å¯ä»¥è¿è¡Œå®Œæ•´çš„pipeline")
        print("   python -m src.services.log_pipeline")
        print("   python -m src.services.knowledge_pipeline")
    elif passed_tests >= total_tests * 0.7:
        print("âœ… å¤§éƒ¨åˆ†ç»„ä»¶å·¥ä½œæ­£å¸¸")
        print("ğŸ’¡ å»ºè®®: æ£€æŸ¥å¤±è´¥çš„ç»„ä»¶å¹¶ä¿®å¤")
    else:
        print("âš ï¸ å¤šä¸ªç»„ä»¶å­˜åœ¨é—®é¢˜")
        print("ğŸ’¡ å»ºè®®: æ£€æŸ¥DockeræœåŠ¡çŠ¶æ€")
        print("   docker-compose ps")


if __name__ == "__main__":
    asyncio.run(main())