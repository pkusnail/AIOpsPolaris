#!/usr/bin/env python3
"""
vLLMæœåŠ¡é›†æˆæµ‹è¯•è„šæœ¬
æµ‹è¯•vLLM OpenAIå…¼å®¹APIæ¥å£
"""

import asyncio
import aiohttp
import json
import time
from typing import Dict, Any, List

# vLLMæœåŠ¡é…ç½®
VLLM_CONFIG = {
    "base_url": "http://localhost:8000/v1",
    "model": "Qwen/Qwen2.5-7B-Instruct",
    "timeout": 60
}

async def test_vllm_health(session: aiohttp.ClientSession) -> Dict[str, Any]:
    """æµ‹è¯•vLLMå¥åº·çŠ¶æ€"""
    try:
        async with session.get(f"http://localhost:8000/health") as response:
            if response.status == 200:
                return {"status": "healthy", "details": "Service is running"}
            else:
                return {"status": "unhealthy", "details": f"HTTP {response.status}"}
    except Exception as e:
        return {"status": "error", "details": str(e)}

async def test_vllm_models(session: aiohttp.ClientSession) -> Dict[str, Any]:
    """æµ‹è¯•vLLMæ¨¡å‹åˆ—è¡¨æ¥å£"""
    try:
        async with session.get(f"{VLLM_CONFIG['base_url']}/models") as response:
            if response.status == 200:
                data = await response.json()
                models = [model["id"] for model in data.get("data", [])]
                return {
                    "status": "success",
                    "models": models,
                    "model_count": len(models)
                }
            else:
                return {"status": "error", "details": f"HTTP {response.status}"}
    except Exception as e:
        return {"status": "error", "details": str(e)}

async def test_vllm_completions(session: aiohttp.ClientSession) -> Dict[str, Any]:
    """æµ‹è¯•vLLMæ–‡æœ¬ç”Ÿæˆæ¥å£"""
    try:
        # æµ‹è¯•æç¤ºè¯
        prompts = [
            {
                "name": "ç®€å•é—®ç­”",
                "prompt": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
                "max_tokens": 100
            },
            {
                "name": "AIOpsåœºæ™¯",
                "prompt": "ä½œä¸ºä¸€ä¸ªAIOpsä¸“å®¶ï¼Œè¯·è§£é‡Šä»€ä¹ˆæ˜¯å¼‚å¸¸æ£€æµ‹ï¼Ÿ",
                "max_tokens": 200
            },
            {
                "name": "ä¸­æ–‡æ¨ç†",
                "prompt": "å¦‚æœä¸€ä¸ªç³»ç»Ÿçš„CPUä½¿ç”¨ç‡çªç„¶ä»20%å‡é«˜åˆ°90%ï¼Œå¯èƒ½çš„åŸå› æœ‰å“ªäº›ï¼Ÿ",
                "max_tokens": 150
            }
        ]
        
        results = []
        
        for test_case in prompts:
            print(f"      æµ‹è¯•: {test_case['name']}")
            
            payload = {
                "model": VLLM_CONFIG["model"],
                "prompt": test_case["prompt"],
                "max_tokens": test_case["max_tokens"],
                "temperature": 0.7,
                "stop": ["Human:", "\n\n"]
            }
            
            start_time = time.time()
            
            timeout = aiohttp.ClientTimeout(total=VLLM_CONFIG["timeout"])
            async with session.post(
                f"{VLLM_CONFIG['base_url']}/completions",
                json=payload,
                timeout=timeout
            ) as response:
                
                if response.status == 200:
                    data = await response.json()
                    response_time = time.time() - start_time
                    
                    choice = data["choices"][0]
                    result = {
                        "test_name": test_case["name"],
                        "status": "success",
                        "response_time": round(response_time, 2),
                        "generated_text": choice["text"].strip(),
                        "finish_reason": choice["finish_reason"],
                        "tokens_used": data["usage"]["total_tokens"]
                    }
                    results.append(result)
                    print(f"        âœ… æˆåŠŸ ({response_time:.2f}s, {data['usage']['total_tokens']} tokens)")
                    
                else:
                    error_text = await response.text()
                    result = {
                        "test_name": test_case["name"],
                        "status": "error",
                        "details": f"HTTP {response.status}: {error_text}"
                    }
                    results.append(result)
                    print(f"        âŒ å¤±è´¥: HTTP {response.status}")
        
        return {
            "status": "success" if all(r["status"] == "success" for r in results) else "partial",
            "test_results": results,
            "successful_tests": sum(1 for r in results if r["status"] == "success"),
            "total_tests": len(results)
        }
        
    except Exception as e:
        return {"status": "error", "details": str(e)}

async def test_vllm_chat_completions(session: aiohttp.ClientSession) -> Dict[str, Any]:
    """æµ‹è¯•vLLMèŠå¤©æ¥å£"""
    try:
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIOpsåŠ©æ‰‹ï¼Œæ“…é•¿ç³»ç»Ÿè¿ç»´å’Œæ•…éšœè¯Šæ–­ã€‚"
            },
            {
                "role": "user",
                "content": "æˆ‘çš„æœåŠ¡å™¨å†…å­˜ä½¿ç”¨ç‡è¾¾åˆ°95%ï¼Œåº”è¯¥æ€ä¹ˆå¤„ç†ï¼Ÿ"
            }
        ]
        
        payload = {
            "model": VLLM_CONFIG["model"],
            "messages": messages,
            "max_tokens": 200,
            "temperature": 0.7
        }
        
        start_time = time.time()
        
        timeout = aiohttp.ClientTimeout(total=VLLM_CONFIG["timeout"])
        async with session.post(
            f"{VLLM_CONFIG['base_url']}/chat/completions",
            json=payload,
            timeout=timeout
        ) as response:
            
            if response.status == 200:
                data = await response.json()
                response_time = time.time() - start_time
                
                choice = data["choices"][0]
                return {
                    "status": "success",
                    "response_time": round(response_time, 2),
                    "message_content": choice["message"]["content"].strip(),
                    "finish_reason": choice["finish_reason"],
                    "tokens_used": data["usage"]["total_tokens"]
                }
            else:
                error_text = await response.text()
                return {
                    "status": "error", 
                    "details": f"HTTP {response.status}: {error_text}"
                }
    except Exception as e:
        return {"status": "error", "details": str(e)}

async def test_vllm_streaming(session: aiohttp.ClientSession) -> Dict[str, Any]:
    """æµ‹è¯•vLLMæµå¼å“åº”"""
    try:
        payload = {
            "model": VLLM_CONFIG["model"],
            "prompt": "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹æœºå™¨å­¦ä¹ åœ¨è¿ç»´ä¸­çš„åº”ç”¨",
            "max_tokens": 150,
            "stream": True,
            "temperature": 0.7
        }
        
        start_time = time.time()
        chunks_received = 0
        total_content = ""
        
        timeout = aiohttp.ClientTimeout(total=VLLM_CONFIG["timeout"])
        async with session.post(
            f"{VLLM_CONFIG['base_url']}/completions",
            json=payload,
            timeout=timeout
        ) as response:
            
            if response.status == 200:
                async for chunk in response.content:
                    if chunk:
                        chunk_text = chunk.decode('utf-8').strip()
                        if chunk_text.startswith('data: '):
                            data_text = chunk_text[6:]  # Remove 'data: ' prefix
                            if data_text != '[DONE]':
                                try:
                                    chunk_data = json.loads(data_text)
                                    if chunk_data["choices"][0]["text"]:
                                        total_content += chunk_data["choices"][0]["text"]
                                        chunks_received += 1
                                except json.JSONDecodeError:
                                    continue
                
                response_time = time.time() - start_time
                
                return {
                    "status": "success",
                    "response_time": round(response_time, 2),
                    "chunks_received": chunks_received,
                    "total_content_length": len(total_content),
                    "sample_content": total_content[:100] + "..." if len(total_content) > 100 else total_content
                }
            else:
                return {"status": "error", "details": f"HTTP {response.status}"}
                
    except Exception as e:
        return {"status": "error", "details": str(e)}

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¤– AIOps Polaris vLLMæœåŠ¡é›†æˆæµ‹è¯•")
    print("=" * 50)
    
    timeout = aiohttp.ClientTimeout(total=120)
    
    async with aiohttp.ClientSession(timeout=timeout) as session:
        
        # 1. æµ‹è¯•æœåŠ¡å¥åº·çŠ¶æ€
        print("\n1ï¸âƒ£  æµ‹è¯•vLLMæœåŠ¡å¥åº·çŠ¶æ€...")
        health_result = await test_vllm_health(session)
        if health_result["status"] == "healthy":
            print(f"   âœ… vLLMæœåŠ¡æ­£å¸¸è¿è¡Œ")
        else:
            print(f"   âŒ vLLMæœåŠ¡å¼‚å¸¸: {health_result['details']}")
            print("   è¯·ç¡®ä¿vLLMæœåŠ¡å·²å¯åŠ¨å¹¶å®Œæˆæ¨¡å‹åŠ è½½")
            return
        
        # 2. æµ‹è¯•æ¨¡å‹åˆ—è¡¨
        print("\n2ï¸âƒ£  æµ‹è¯•æ¨¡å‹åˆ—è¡¨æ¥å£...")
        models_result = await test_vllm_models(session)
        if models_result["status"] == "success":
            print(f"   âœ… æ‰¾åˆ° {models_result['model_count']} ä¸ªæ¨¡å‹")
            for model in models_result["models"]:
                print(f"      - {model}")
        else:
            print(f"   âŒ æ¨¡å‹åˆ—è¡¨è·å–å¤±è´¥: {models_result['details']}")
        
        # 3. æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
        print("\n3ï¸âƒ£  æµ‹è¯•æ–‡æœ¬ç”Ÿæˆæ¥å£...")
        completions_result = await test_vllm_completions(session)
        if completions_result["status"] in ["success", "partial"]:
            print(f"   âœ… å®Œæˆ {completions_result['successful_tests']}/{completions_result['total_tests']} ä¸ªæµ‹è¯•")
            for test in completions_result["test_results"]:
                if test["status"] == "success":
                    print(f"      ğŸ“ {test['test_name']}: {test['generated_text'][:50]}...")
        else:
            print(f"   âŒ æ–‡æœ¬ç”Ÿæˆæµ‹è¯•å¤±è´¥: {completions_result['details']}")
        
        # 4. æµ‹è¯•èŠå¤©æ¥å£
        print("\n4ï¸âƒ£  æµ‹è¯•èŠå¤©æ¥å£...")
        chat_result = await test_vllm_chat_completions(session)
        if chat_result["status"] == "success":
            print(f"   âœ… èŠå¤©æ¥å£æµ‹è¯•æˆåŠŸ ({chat_result['response_time']}s)")
            print(f"      ğŸ’¬ å›å¤: {chat_result['message_content'][:100]}...")
        else:
            print(f"   âŒ èŠå¤©æ¥å£æµ‹è¯•å¤±è´¥: {chat_result['details']}")
        
        # 5. æµ‹è¯•æµå¼å“åº”
        print("\n5ï¸âƒ£  æµ‹è¯•æµå¼å“åº”...")
        streaming_result = await test_vllm_streaming(session)
        if streaming_result["status"] == "success":
            print(f"   âœ… æµå¼å“åº”æµ‹è¯•æˆåŠŸ")
            print(f"      ğŸ“Š æ¥æ”¶ {streaming_result['chunks_received']} ä¸ªæ•°æ®å—")
            print(f"      ğŸ“ ç¤ºä¾‹å†…å®¹: {streaming_result['sample_content']}")
        else:
            print(f"   âŒ æµå¼å“åº”æµ‹è¯•å¤±è´¥: {streaming_result['details']}")
    
    print("\nğŸ¯ vLLMæµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“‹ GPUä½¿ç”¨æƒ…å†µ:")
    
    # æ˜¾ç¤ºGPUçŠ¶æ€
    try:
        import subprocess
        result = subprocess.run(["nvidia-smi", "--query-gpu=name,memory.used,memory.total,utilization.gpu", "--format=csv,noheader,nounits"], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            gpu_info = result.stdout.strip().split(', ')
            print(f"   GPU: {gpu_info[0]}")
            print(f"   æ˜¾å­˜ä½¿ç”¨: {gpu_info[1]}MB / {gpu_info[2]}MB")
            print(f"   GPUåˆ©ç”¨ç‡: {gpu_info[3]}%")
        else:
            print("   æ— æ³•è·å–GPUä¿¡æ¯")
    except Exception as e:
        print(f"   GPUä¿¡æ¯è·å–å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    asyncio.run(main())