# AIOps Polaris æŠ€æœ¯æŒ‡å—

## ğŸ“– ç›®å½•
1. [ç³»ç»Ÿæ¦‚è¿°](#ç³»ç»Ÿæ¦‚è¿°)
2. [æ ¸å¿ƒç»„ä»¶è¯¦è§£](#æ ¸å¿ƒç»„ä»¶è¯¦è§£)
3. [Multi-Agentå·¥ä½œæœºåˆ¶](#multi-agentå·¥ä½œæœºåˆ¶)
4. [RAGæœç´¢å¼•æ“å®ç°](#ragæœç´¢å¼•æ“å®ç°)
5. [æ•°æ®æµå’ŒçŠ¶æ€ç®¡ç†](#æ•°æ®æµå’ŒçŠ¶æ€ç®¡ç†)
6. [æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](#æ€§èƒ½ä¼˜åŒ–ç­–ç•¥)
7. [éƒ¨ç½²å’Œè¿ç»´](#éƒ¨ç½²å’Œè¿ç»´)
8. [å¼€å‘æŒ‡å—](#å¼€å‘æŒ‡å—)

---

## ğŸ¯ ç³»ç»Ÿæ¦‚è¿°

AIOps Polaris æ˜¯ä¸€ä¸ªåŸºäºå¤šæ™ºèƒ½ä½“åä½œçš„æ™ºèƒ½è¿ç»´å¹³å°ï¼Œæ ¸å¿ƒç‰¹ç‚¹ï¼š

### å…³é”®è®¾è®¡åŸåˆ™
- **å¼‚æ­¥ä¼˜å…ˆ**: å…¨é¢é‡‡ç”¨Python asyncioå®ç°é«˜å¹¶å‘
- **æ¾è€¦åˆæ¶æ„**: å¾®æœåŠ¡è®¾è®¡ï¼Œç»„ä»¶é—´é€šè¿‡æ¶ˆæ¯ä¼ é€’
- **çŠ¶æ€é©±åŠ¨**: åŸºäºçŠ¶æ€æœºçš„ä»»åŠ¡ç®¡ç†å’ŒAgentåä½œ
- **å¯è§‚æµ‹æ€§**: å…¨é“¾è·¯è¿½è¸ªå’Œå®æ—¶çŠ¶æ€ç›‘æ§
- **ç”¨æˆ·ä½“éªŒ**: å®æ—¶åé¦ˆå’Œäº¤äº’å¼æ§åˆ¶

### æ ¸å¿ƒæŠ€æœ¯å†³ç­–
```yaml
ç¼–ç¨‹è¯­è¨€: Python 3.9+ (å¼‚æ­¥æ”¯æŒå®Œå–„)
Webæ¡†æ¶: FastAPI (é«˜æ€§èƒ½ï¼Œè‡ªåŠ¨æ–‡æ¡£ç”Ÿæˆ)
AI/MLæ¡†æ¶: OpenAI API + SentenceTransformers (å¹³è¡¡æ•ˆæœå’Œæˆæœ¬)
æ•°æ®å­˜å‚¨: Weaviate(å‘é‡) + Neo4j(å›¾) + Redis(ç¼“å­˜)
å‰ç«¯: åŸç”ŸJavaScript + é•¿è½®è¯¢ (ç®€å•å¯é ï¼Œé¿å…WebSocketå¤æ‚æ€§)
å®¹å™¨åŒ–: Docker + Docker Compose (ç®€åŒ–éƒ¨ç½²)
```

---

## ğŸ”§ æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. Enhanced Streaming RCA Service
**æ ¸å¿ƒåè°ƒå™¨ - æ•´ä¸ªMulti-Agentç³»ç»Ÿçš„å¤§è„‘**

```python
# æ–‡ä»¶: src/api/enhanced_streaming_rca_service.py
class EnhancedStreamingRCAService:
    """Enhancedæµå¼RCAåˆ†ææœåŠ¡ - Multi-Agentåè°ƒæ ¸å¿ƒ"""
    
    def __init__(self):
        self.task_manager = multi_agent_task_manager
        self.running_tasks: Dict[str, asyncio.Task] = {}
        
    async def start_multi_agent_analysis(self, message: str, user_id: str) -> str:
        """å¯åŠ¨Multi-Agentåˆ†ææµç¨‹"""
        # 1. åˆ›å»ºä»»åŠ¡
        task_id = self.task_manager.create_multi_agent_task(user_id, message)
        
        # 2. å¼‚æ­¥æ‰§è¡ŒAgentåä½œæµç¨‹
        async_task = asyncio.create_task(
            self._execute_multi_agent_workflow(task_id, message)
        )
        self.running_tasks[task_id] = async_task
        
        return task_id
    
    async def _execute_multi_agent_workflow(self, task_id: str, message: str):
        """æ‰§è¡ŒMulti-Agentå·¥ä½œæµ"""
        try:
            # Phase 1: Planning - Planner Agent
            await self._planning_phase(task_id, message)
            
            # Phase 2: Knowledge Collection - Knowledge Agent  
            await self._knowledge_collection_phase(task_id, message)
            
            # Phase 3: Reasoning - Reasoning Agent
            await self._reasoning_phase(task_id, message)
            
            # Phase 4: Solution Generation - Executor Agent
            await self._execution_phase(task_id, message)
            
            # Phase 5: Result Integration
            await self._result_integration_phase(task_id)
            
        except Exception as e:
            self.task_manager.fail_multi_agent_task(task_id, str(e))
```

**å…³é”®è®¾è®¡ç‚¹**:
- ä½¿ç”¨`asyncio.create_task`å®ç°çœŸæ­£çš„å¹¶å‘æ‰§è¡Œ
- æ¯ä¸ªAgenté˜¶æ®µéƒ½æœ‰è¯¦ç»†çš„çŠ¶æ€è·Ÿè¸ª
- æ”¯æŒç”¨æˆ·ä¸­æ–­å’Œé”™è¯¯æ¢å¤
- å®ç°äº†èƒŒå‹æ§åˆ¶ï¼Œé˜²æ­¢ç³»ç»Ÿè¿‡è½½

### 2. Multi-Agent Task Manager
**ä»»åŠ¡çŠ¶æ€ç®¡ç†å™¨ - å®æ—¶çŠ¶æ€è·Ÿè¸ªå’Œç”¨æˆ·äº¤äº’**

```python
# æ–‡ä»¶: src/utils/multi_agent_task_manager.py
@dataclass
class MultiAgentTaskInfo:
    """Multi-Agentä»»åŠ¡å®Œæ•´çŠ¶æ€"""
    task_id: str
    status: str  # queued, planning, executing, completed, failed, interrupted
    overall_progress: float  # 0.0 - 1.0
    current_phase: str  # planning, execution, review
    user_can_interrupt: bool
    
    # Planningç›¸å…³
    planning_sessions: List[PlanningSession]
    current_plan_version: int
    
    # AgentçŠ¶æ€  
    agents: Dict[str, AgentStatus]  # agent_id -> status
    
    # ä¸­é—´ç»“æœ
    intermediate_conclusions: List[Dict[str, Any]]
    final_result: Optional[Dict[str, Any]] = None

class MultiAgentTaskManager:
    def __init__(self):
        # ç³»ç»Ÿä¸­çš„æ‰€æœ‰å¯ç”¨agents
        self.available_agents = {
            "planner": {"name": "è§„åˆ’æ™ºèƒ½ä½“", "description": "åˆ†æé—®é¢˜å¹¶åˆ¶å®šæ‰§è¡Œè®¡åˆ’"},
            "knowledge": {"name": "çŸ¥è¯†æ™ºèƒ½ä½“", "description": "æœç´¢å’Œæ•´ç†ç›¸å…³çŸ¥è¯†"},
            "reasoning": {"name": "æ¨ç†æ™ºèƒ½ä½“", "description": "è¿›è¡Œé€»è¾‘æ¨ç†å’Œæ ¹å› åˆ†æ"},
            "executor": {"name": "æ‰§è¡Œæ™ºèƒ½ä½“", "description": "ç”Ÿæˆå…·ä½“çš„è§£å†³æ–¹æ¡ˆ"},
            "monitor": {"name": "ç›‘æ§æ™ºèƒ½ä½“", "description": "ç›‘æ§æ‰§è¡Œè¿‡ç¨‹å’Œç»“æœéªŒè¯"}
        }
    
    def _update_overall_progress(self, task_id: str):
        """æ™ºèƒ½è¿›åº¦è®¡ç®—ç®—æ³•"""
        task_info = self.tasks[task_id]
        
        # åŠ æƒè¿›åº¦è®¡ç®— - ä¸åŒAgentæœ‰ä¸åŒé‡è¦æ€§
        agent_weights = {
            "planner": 0.15,      # è§„åˆ’ 15%
            "knowledge": 0.40,    # çŸ¥è¯†æ”¶é›†æ˜¯æœ€é‡è¦çš„é˜¶æ®µ 40%
            "reasoning": 0.25,    # æ¨ç†åˆ†æ 25% 
            "executor": 0.20      # è§£å†³æ–¹æ¡ˆ 20%
        }
        
        total_progress = 0.0
        for agent_id, agent in task_info.agents.items():
            weight = agent_weights.get(agent_id, 1.0/len(task_info.agents))
            if agent.status == "done":
                total_progress += weight * 1.0
            elif agent.status == "working":
                total_progress += weight * agent.progress
        
        task_info.overall_progress = min(total_progress, 1.0)
```

**è®¾è®¡äº®ç‚¹**:
- é‡‡ç”¨åŠ æƒè¿›åº¦ç®—æ³•ï¼ŒKnowledge Agentæƒé‡æœ€é«˜ 
- æ”¯æŒå¤šç‰ˆæœ¬è§„åˆ’ï¼Œå¯ä»¥åŠ¨æ€è°ƒæ•´æ‰§è¡Œè®¡åˆ’
- ç»†ç²’åº¦çš„AgentçŠ¶æ€ç®¡ç†ï¼ˆwaiting â†’ working â†’ done/failedï¼‰
- å®Œæ•´çš„ä¸­é—´ç»“è®ºè®°å½•ï¼Œæ”¯æŒè°ƒè¯•å’Œå®¡è®¡

### 3. Improved RAG Service
**æ··åˆæœç´¢å¼•æ“ - è¯­ä¹‰æœç´¢+å…³é”®è¯æœç´¢çš„å®Œç¾ç»“åˆ**

```python
# æ–‡ä»¶: src/services/improved_rag_service.py
class ImprovedRAGService:
    """æ”¹è¿›çš„RAGæœç´¢æœåŠ¡ - æ··åˆæ£€ç´¢ç­–ç•¥"""
    
    async def hybrid_search(self, query: str, limit: int = 15) -> Dict[str, Any]:
        """æ··åˆæœç´¢ - æ ¸å¿ƒç®—æ³•"""
        # å¹¶è¡Œæ‰§è¡Œå‘é‡æœç´¢å’ŒBM25æœç´¢
        vector_task = asyncio.create_task(self.vector_search(query, limit))
        bm25_task = asyncio.create_task(self.bm25_search(query, limit))
        
        vector_results, bm25_results = await asyncio.gather(vector_task, bm25_task)
        
        # æ··åˆé‡æ’åºç®—æ³•
        merged_results = self.rerank_results(
            vector_results, bm25_results, query, alpha=0.6
        )
        
        return {
            "results": merged_results,
            "vector_count": len(vector_results),
            "bm25_count": len(bm25_results),
            "total_count": len(merged_results)
        }
    
    def rerank_results(self, vector_results, bm25_results, query, alpha=0.6):
        """æ··åˆé‡æ’åºç®—æ³•å®ç°"""
        all_results = {}
        
        # åˆå¹¶ç»“æœï¼Œå¤„ç†é‡å¤
        for doc in vector_results:
            content = doc.get('content', '')
            if content not in all_results:
                all_results[content] = doc.copy()
                all_results[content]['vector_score'] = doc.get('score', 0.0)
                all_results[content]['bm25_score'] = 0.0
        
        for doc in bm25_results:
            content = doc.get('content', '')
            if content in all_results:
                all_results[content]['bm25_score'] = doc.get('score', 0.0)
            else:
                all_results[content] = doc.copy()
                all_results[content]['vector_score'] = 0.0
                all_results[content]['bm25_score'] = doc.get('score', 0.0)
        
        # æ··åˆå¾—åˆ†è®¡ç®—å’Œæ’åº
        for content, doc in all_results.items():
            vector_score = min(doc['vector_score'], 1.0)
            bm25_score = min(doc['bm25_score'] / 3.0, 1.0)  # BM25å½’ä¸€åŒ–
            
            # åŠ æƒæ··åˆå¾—åˆ†: Î±Ã—è¯­ä¹‰ç›¸ä¼¼åº¦ + (1-Î±)Ã—å…³é”®è¯åŒ¹é…åº¦
            hybrid_score = alpha * vector_score + (1 - alpha) * bm25_score
            doc['hybrid_score'] = hybrid_score
            doc['search_type'] = 'hybrid'
        
        # æŒ‰æ··åˆå¾—åˆ†æ’åºè¿”å›
        return sorted(all_results.values(), key=lambda x: x['hybrid_score'], reverse=True)
```

**ç®—æ³•ä¼˜åŒ–**:
- `Î±=0.6`: è¯­ä¹‰æœç´¢æƒé‡æ›´é«˜ï¼Œé€‚åˆå¤æ‚æŠ€æœ¯é—®é¢˜
- BM25å¾—åˆ†å½’ä¸€åŒ–ï¼š`score/3.0`ï¼ŒåŸºäºå®é™…æ•°æ®åˆ†å¸ƒè°ƒä¼˜
- å†…å®¹å»é‡ï¼šé¿å…åŒä¸€æ–‡æ¡£åœ¨ä¸¤ç§æœç´¢ä¸­é‡å¤
- æ¸è¿›å¼å›é€€ï¼šå‘é‡æœç´¢å¤±è´¥æ—¶è‡ªåŠ¨é™çº§åˆ°BM25

---

## ğŸ¤– Multi-Agentå·¥ä½œæœºåˆ¶

### Agentåä½œçŠ¶æ€æœº
```mermaid
stateDiagram-v2
    [*] --> TaskCreated
    TaskCreated --> PlannerWorking: start_planning()
    
    state PlannerWorking {
        [*] --> AnalyzingProblem
        AnalyzingProblem --> CreatingPlan
        CreatingPlan --> ValidatingPlan
        ValidatingPlan --> [*]: plan_ready
    }
    
    PlannerWorking --> KnowledgeWorking: plan_completed
    
    state KnowledgeWorking {
        [*] --> EntityRecognition
        EntityRecognition --> EvidenceCollection
        EvidenceCollection --> TopologyAnalysis
        TopologyAnalysis --> [*]: knowledge_ready
    }
    
    KnowledgeWorking --> ReasoningWorking: knowledge_completed
    
    state ReasoningWorking {
        [*] --> PatternAnalysis
        PatternAnalysis --> CausalInference
        CausalInference --> ConfidenceCalculation
        ConfidenceCalculation --> [*]: reasoning_ready
    }
    
    ReasoningWorking --> ExecutorWorking: reasoning_completed
    
    state ExecutorWorking {
        [*] --> SolutionGeneration
        SolutionGeneration --> PriorityRanking
        PriorityRanking --> ValidationCheck
        ValidationCheck --> [*]: solutions_ready
    }
    
    ExecutorWorking --> TaskCompleted: execution_completed
    TaskCompleted --> [*]
    
    PlannerWorking --> TaskInterrupted: user_interrupt
    KnowledgeWorking --> TaskInterrupted: user_interrupt
    ReasoningWorking --> TaskInterrupted: user_interrupt
    ExecutorWorking --> TaskInterrupted: user_interrupt
    
    TaskInterrupted --> [*]
```

### å…·ä½“Agentå®ç°ç»†èŠ‚

#### 1. Knowledge Agent - ä¸‰é˜¶æ®µçŸ¥è¯†æ”¶é›†
```python
async def _knowledge_collection_phase(self, task_id: str, message: str):
    """Knowledge Agentæ‰§è¡Œæµç¨‹"""
    
    # Step 1: å®ä½“è¯†åˆ« (NER)
    await self._execute_step(task_id, "å®ä½“è¯†åˆ«", "knowledge",
                           lambda: self._ner_extraction(message))
    
    # Step 2: è¯æ®æ”¶é›† (RAGæ··åˆæœç´¢)
    await self._execute_step(task_id, "è¯æ®æ”¶é›†", "knowledge",  
                           lambda: self._evidence_collection(message))
    
    # Step 3: æ‹“æ‰‘åˆ†æ (Neo4jæŸ¥è¯¢)
    await self._execute_step(task_id, "æ‹“æ‰‘åˆ†æ", "knowledge",
                           lambda: self._topology_analysis(message))

async def _evidence_collection(self, message: str) -> Dict[str, Any]:
    """è¯æ®æ”¶é›† - RAGæœç´¢å®ç°"""
    improved_rag = ImprovedRAGService()
    
    # å…³é”®ï¼šä½¿ç”¨æ··åˆæœç´¢è·å–æ›´å…¨é¢çš„è¯æ®
    search_results = await improved_rag.hybrid_search(query=message, limit=15)
    actual_results = search_results.get("results", [])
    
    return {
        "summary": f"æ”¶é›†åˆ°{len(actual_results)}ä¸ªè¯æ®æ–‡æ¡£",
        "conclusion": f"ä»æ—¥å¿—å’Œæ–‡æ¡£ä¸­æ‰¾åˆ°{len(actual_results)}æ¡ç›¸å…³è¯æ®",
        "confidence": 0.85,
        "data": {
            "evidence_count": len(actual_results),
            "evidence": actual_results[:5],        # UIæ˜¾ç¤ºå‰5ä¸ª
            "full_search_results": actual_results  # å®Œæ•´ç»“æœä¾›åç»­åˆ†æ
        }
    }
```

#### 2. Reasoning Agent - å¤šç»´åº¦åˆ†æ
```python
async def _root_cause_reasoning(self, message: str) -> Dict[str, Any]:
    """æ ¹å› æ¨ç† - å¤šç»´åº¦åˆ†ææ–¹æ³•"""
    
    # æ¨¡æ‹Ÿå¤æ‚çš„æ¨ç†è¿‡ç¨‹
    reasoning_steps = [
        "åˆ†æç—‡çŠ¶æ¨¡å¼å’Œæ—¶é—´åºåˆ—",      # æ—¶é—´ç»´åº¦
        "å…³è”ä¾èµ–æœåŠ¡çŠ¶æ€å˜åŒ–",        # ç©ºé—´ç»´åº¦  
        "è¯†åˆ«æ€§èƒ½ç“¶é¢ˆå’Œèµ„æºçº¦æŸ",      # èµ„æºç»´åº¦
        "ç¡®å®šæœ€å¯èƒ½çš„æ ¹æœ¬åŸå› "         # å› æœç»´åº¦
    ]
    
    # å®é™…å®ç°ä¸­è¿™é‡Œä¼šè°ƒç”¨æ›´å¤æ‚çš„æ¨ç†é€»è¾‘
    # 1. æ—¶åºåˆ†æï¼šæ£€æŸ¥äº‹ä»¶å‘ç”Ÿçš„æ—¶é—´æ¨¡å¼
    # 2. ä¾èµ–åˆ†æï¼šåŸºäºNeo4jæ‹“æ‰‘æ•°æ®åˆ†æå½±å“é“¾
    # 3. æ¨¡å¼åŒ¹é…ï¼šä¸å†å²æ•…éšœæ¡ˆä¾‹å¯¹æ¯”
    # 4. ç½®ä¿¡åº¦è®¡ç®—ï¼šåŸºäºè¯æ®å¼ºåº¦è®¡ç®—å¯ä¿¡åº¦
    
    return {
        "summary": "å®Œæˆå¤šç»´åº¦æ ¹å› åˆ†æ", 
        "conclusion": "åŸºäºCPUä½¿ç”¨ç‡å¼‚å¸¸å’ŒæœåŠ¡ä¾èµ–åˆ†æï¼Œåˆ¤æ–­å¯èƒ½æ˜¯èµ„æºç«äº‰å¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆ",
        "confidence": 0.82,
        "data": {"reasoning_steps": reasoning_steps}
    }
```

---

## ğŸ” RAGæœç´¢å¼•æ“å®ç°

### æ•°æ®é¢„å¤„ç†Pipeline
```python
# æ•°æ®é¢„å¤„ç†æµç¨‹
class DataProcessingPipeline:
    """æ•°æ®é¢„å¤„ç†ç®¡é“"""
    
    async def process_document(self, content: str, metadata: Dict) -> List[Dict]:
        """æ–‡æ¡£å¤„ç†æµç¨‹"""
        
        # 1. æ–‡æœ¬æ¸…ç†å’Œæ ‡å‡†åŒ–
        cleaned_content = self.clean_text(content)
        
        # 2. åˆ†å—ç­–ç•¥ - é‡è¦çš„æ€§èƒ½ä¼˜åŒ–ç‚¹
        chunks = self.chunk_text(
            cleaned_content, 
            chunk_size=512,      # åŸºäºSentenceTransformersæ¨¡å‹é™åˆ¶
            overlap=50           # ä¿è¯ä¸Šä¸‹æ–‡è¿ç»­æ€§
        )
        
        # 3. å‘é‡åŒ–
        embeddings = []
        for chunk in chunks:
            embedding = await self.embedding_service.encode(chunk)
            embeddings.append(embedding)
        
        # 4. å­˜å‚¨åˆ°Weaviate
        documents = []
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            doc = {
                "content": chunk,
                "source_type": metadata.get("source_type"),
                "service_name": metadata.get("service_name"),
                "log_file": metadata.get("log_file"), 
                "timestamp": metadata.get("timestamp"),
                "chunk_index": i,
                "vector": embedding
            }
            documents.append(doc)
        
        return documents

    def chunk_text(self, text: str, chunk_size: int = 512, overlap: int = 50) -> List[str]:
        """æ™ºèƒ½æ–‡æœ¬åˆ†å—ç®—æ³•"""
        # æŒ‰å¥å­è¾¹ç•Œåˆ†å—ï¼Œé¿å…æˆªæ–­é‡è¦ä¿¡æ¯
        sentences = self.split_sentences(text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk + sentence) > chunk_size:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    # ä¿ç•™overlapéƒ¨åˆ†ä½œä¸ºä¸‹ä¸€ä¸ªchunkçš„å¼€å§‹
                    current_chunk = current_chunk[-overlap:] + sentence
                else:
                    # å•ä¸ªå¥å­è¶…é•¿ï¼Œå¼ºåˆ¶åˆ†å‰²
                    current_chunk = sentence[:chunk_size]
            else:
                current_chunk += sentence
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
```

### Weaviate Schemaä¼˜åŒ–
```python
# ä¼˜åŒ–çš„Weaviate Schemaè®¾è®¡
EMBEDDING_COLLECTION_SCHEMA = {
    "class": "EmbeddingCollection",
    "description": "ä¼˜åŒ–çš„å‘é‡æœç´¢é›†åˆ",
    "vectorIndexConfig": {
        "skip": False,
        "cleanupIntervalSeconds": 300,
        "maxConnections": 64,          # HNSWå›¾çš„æœ€å¤§è¿æ¥æ•°
        "efConstruction": 128,         # æ„å»ºæ—¶çš„æœç´¢å®½åº¦
        "ef": -1,                      # åŠ¨æ€è°ƒæ•´æœç´¢å®½åº¦
        "dynamicEfMin": 100,           # æœ€å°æœç´¢å®½åº¦
        "dynamicEfMax": 500,           # æœ€å¤§æœç´¢å®½åº¦
        "dynamicEfFactor": 8,          # åŠ¨æ€è°ƒæ•´å› å­
        "vectorCacheMaxObjects": 1e12, # å‘é‡ç¼“å­˜å¤§å°
        "flatSearchCutoff": 40000,     # å¹³å¦æœç´¢åˆ‡æ¢ç‚¹
        "distance": "cosine"           # ä½™å¼¦ç›¸ä¼¼åº¦
    },
    "invertedIndexConfig": {
        "bm25": {
            "k1": 1.2,    # è¯é¢‘é¥±å’Œå‚æ•°
            "b": 0.75     # é•¿åº¦å½’ä¸€åŒ–å‚æ•°
        }
    }
}
```

---

## ğŸ“Š æ•°æ®æµå’ŒçŠ¶æ€ç®¡ç†

### å®æ—¶çŠ¶æ€åŒæ­¥æœºåˆ¶
```javascript
// å‰ç«¯é•¿è½®è¯¢å®ç°
class RealTimeTaskMonitor {
    constructor(taskId) {
        this.taskId = taskId;
        this.pollInterval = 500;  // 500msè½®è¯¢é—´éš”
        this.maxRetries = 3;
        this.retryCount = 0;
    }
    
    async startMonitoring() {
        while (this.isActive) {
            try {
                const status = await this.fetchTaskStatus();
                this.updateUI(status);
                
                // ä»»åŠ¡å®Œæˆæˆ–å¤±è´¥æ—¶åœæ­¢è½®è¯¢
                if (['completed', 'failed', 'interrupted'].includes(status.status)) {
                    this.stop();
                    break;
                }
                
                await this.sleep(this.pollInterval);
                this.retryCount = 0;  // é‡ç½®é‡è¯•è®¡æ•°
                
            } catch (error) {
                this.handleError(error);
            }
        }
    }
    
    handleError(error) {
        this.retryCount++;
        if (this.retryCount >= this.maxRetries) {
            this.showError("è¿æ¥ä¸­æ–­ï¼Œè¯·åˆ·æ–°é¡µé¢é‡è¯•");
            this.stop();
        } else {
            // æŒ‡æ•°é€€é¿é‡è¯•
            this.pollInterval = Math.min(this.pollInterval * 2, 5000);
        }
    }
    
    updateUI(status) {
        // æ›´æ–°è¿›åº¦æ¡
        this.updateProgress(status.overall_progress);
        
        // æ›´æ–°AgentçŠ¶æ€
        this.renderAgentStatus(status.agents);
        
        // æ˜¾ç¤ºä¸­é—´ç»“è®º
        this.showIntermediateResults(status.intermediate_conclusions);
        
        // æ›´æ–°é˜¶æ®µä¿¡æ¯
        this.updatePhaseIndicator(status.current_phase);
    }
}
```

### é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥
```python
class ErrorHandlingMixin:
    """é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥"""
    
    async def execute_with_fallback(self, primary_func, fallback_func, context=""):
        """æ‰§è¡Œä¸»è¦åŠŸèƒ½ï¼Œå¤±è´¥æ—¶é™çº§åˆ°å¤‡ç”¨æ–¹æ¡ˆ"""
        try:
            return await primary_func()
        except Exception as e:
            logger.warning(f"{context} ä¸»è¦æ–¹æ¡ˆå¤±è´¥: {e}, å°è¯•é™çº§æ–¹æ¡ˆ")
            try:
                return await fallback_func()
            except Exception as fe:
                logger.error(f"{context} é™çº§æ–¹æ¡ˆä¹Ÿå¤±è´¥: {fe}")
                raise
    
    async def vector_search_with_fallback(self, query: str, limit: int):
        """å‘é‡æœç´¢é™çº§ç­–ç•¥"""
        async def primary_search():
            return await self.vector_search(query, limit)
        
        async def fallback_search():  
            # é™çº§åˆ°çº¯BM25æœç´¢
            logger.info("å‘é‡æœç´¢å¤±è´¥ï¼Œé™çº§åˆ°BM25æœç´¢")
            return await self.bm25_search(query, limit)
        
        return await self.execute_with_fallback(
            primary_search, fallback_search, "RAGæœç´¢"
        )
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. å¼‚æ­¥I/Oä¼˜åŒ–
```python
# å¹¶å‘ä¼˜åŒ–ç¤ºä¾‹
class PerformanceOptimizer:
    def __init__(self):
        # è¿æ¥æ± ç®¡ç†
        self.weaviate_client = weaviate.Client(
            url="http://localhost:8080",
            timeout_config=(5, 15),  # è¿æ¥è¶…æ—¶5sï¼Œè¯»å–è¶…æ—¶15s
        )
        
        # å¼‚æ­¥HTTPå®¢æˆ·ç«¯
        self.http_session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            connector=aiohttp.TCPConnector(
                limit=100,           # æœ€å¤§è¿æ¥æ•°
                limit_per_host=30,   # æ¯ä¸ªhostçš„æœ€å¤§è¿æ¥æ•°
                ttl_dns_cache=300,   # DNSç¼“å­˜5åˆ†é’Ÿ
                use_dns_cache=True,
            )
        )
    
    async def parallel_data_collection(self, query: str):
        """å¹¶è¡Œæ•°æ®æ”¶é›† - å…³é”®æ€§èƒ½ä¼˜åŒ–ç‚¹"""
        # åŒæ—¶æ‰§è¡Œæ‰€æœ‰æ•°æ®æ”¶é›†ä»»åŠ¡
        tasks = [
            self.ner_extraction(query),           # NERå®ä½“è¯†åˆ«
            self.rag_search(query, limit=15),     # RAGæ··åˆæœç´¢
            self.topology_query(query),           # Neo4jæ‹“æ‰‘æŸ¥è¯¢
            self.historical_analysis(query)       # å†å²æ¡ˆä¾‹åˆ†æ
        ]
        
        # gather()å¹¶è¡Œæ‰§è¡Œï¼Œå¤§å¹…å‡å°‘æ€»è€—æ—¶
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        processed_results = {}
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Task {i} failed: {result}")
                processed_results[f"task_{i}"] = None
            else:
                processed_results[f"task_{i}"] = result
        
        return processed_results
```

### 2. ç¼“å­˜ç­–ç•¥
```python
class IntelligentCaching:
    """æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)
        self.local_cache = {}  # è¿›ç¨‹å†…ç¼“å­˜
        
    async def cached_embedding_generation(self, text: str) -> List[float]:
        """Embeddingç”Ÿæˆç¼“å­˜"""
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"embed:{hashlib.md5(text.encode()).hexdigest()}"
        
        # 1. æ£€æŸ¥è¿›ç¨‹å†…ç¼“å­˜ (æœ€å¿«)
        if cache_key in self.local_cache:
            return self.local_cache[cache_key]
        
        # 2. æ£€æŸ¥Redisç¼“å­˜ (å¿«)
        cached = await self.redis_client.get(cache_key)
        if cached:
            embedding = pickle.loads(cached)
            self.local_cache[cache_key] = embedding  # æ›´æ–°è¿›ç¨‹å†…ç¼“å­˜
            return embedding
        
        # 3. ç”Ÿæˆæ–°çš„embedding (æ…¢)
        embedding = await self.embedding_service.encode(text)
        
        # 4. æ›´æ–°ç¼“å­˜
        self.local_cache[cache_key] = embedding
        await self.redis_client.setex(
            cache_key, 
            3600 * 24 * 7,  # 7å¤©è¿‡æœŸ
            pickle.dumps(embedding)
        )
        
        return embedding
    
    async def cached_search_results(self, query: str, limit: int) -> Dict:
        """æœç´¢ç»“æœç¼“å­˜"""
        cache_key = f"search:{hashlib.md5(f'{query}:{limit}'.encode()).hexdigest()}"
        
        # æ£€æŸ¥ç¼“å­˜
        cached = await self.redis_client.get(cache_key)
        if cached:
            return json.loads(cached)
        
        # æ‰§è¡Œæœç´¢
        results = await self.hybrid_search(query, limit)
        
        # ç¼“å­˜ç»“æœ (çŸ­æœŸç¼“å­˜ï¼Œé¿å…æ•°æ®è¿‡æ—¶)
        await self.redis_client.setex(
            cache_key,
            300,  # 5åˆ†é’Ÿè¿‡æœŸ
            json.dumps(results, default=str)
        )
        
        return results
```

### 3. å†…å­˜ä¼˜åŒ–
```python
class MemoryOptimizer:
    """å†…å­˜ä½¿ç”¨ä¼˜åŒ–"""
    
    def __init__(self):
        # ä½¿ç”¨__slots__å‡å°‘å¯¹è±¡å†…å­˜å¼€é”€
        pass
    
    @lru_cache(maxsize=1000)
    def cached_text_processing(self, text: str) -> str:
        """æ–‡æœ¬å¤„ç†ç»“æœç¼“å­˜"""
        return self.clean_and_normalize_text(text)
    
    def batch_vector_operations(self, texts: List[str], batch_size: int = 32):
        """æ‰¹é‡å‘é‡æ“ä½œï¼Œå‡å°‘å†…å­˜å³°å€¼"""
        results = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_embeddings = self.model.encode(batch)
            results.extend(batch_embeddings)
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾ä¸­é—´ç»“æœå†…å­˜
            import gc
            gc.collect()
        
        return results
```

---

## ğŸš€ éƒ¨ç½²å’Œè¿ç»´

### DockeråŒ–éƒ¨ç½²
```dockerfile
# ç”Ÿäº§ç¯å¢ƒDockerfile
FROM python:3.9-slim

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºåº”ç”¨ç”¨æˆ·
RUN useradd --create-home --shell /bin/bash app
WORKDIR /home/app

# å®‰è£…Pythonä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY --chown=app:app . .

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# ä½¿ç”¨érootç”¨æˆ·è¿è¡Œ
USER app

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### ç›‘æ§å’Œå‘Šè­¦
```yaml
# ç›‘æ§æŒ‡æ ‡é…ç½®
monitoring:
  metrics:
    # ä¸šåŠ¡æŒ‡æ ‡
    - name: "rca_task_completion_rate"
      description: "RCAä»»åŠ¡å®Œæˆç‡"
      threshold: "> 95%"
    
    - name: "agent_average_execution_time"
      description: "Agentå¹³å‡æ‰§è¡Œæ—¶é—´"
      threshold: "< 30s"
    
    - name: "search_response_time_p95"
      description: "æœç´¢å“åº”æ—¶é—´95åˆ†ä½"
      threshold: "< 100ms"
    
    # ç³»ç»ŸæŒ‡æ ‡
    - name: "memory_usage"
      description: "å†…å­˜ä½¿ç”¨ç‡"  
      threshold: "< 80%"
    
    - name: "cpu_usage"
      description: "CPUä½¿ç”¨ç‡"
      threshold: "< 70%"
    
    - name: "disk_usage"
      description: "ç£ç›˜ä½¿ç”¨ç‡"
      threshold: "< 85%"

  alerts:
    - condition: "rca_task_completion_rate < 90%"
      action: "send_email"
      severity: "high"
    
    - condition: "search_response_time_p95 > 200ms"
      action: "send_slack"
      severity: "medium"
```

---

## ğŸ’» å¼€å‘æŒ‡å—

### ä»£ç ç»„ç»‡ç»“æ„
```
src/
â”œâ”€â”€ api/                           # APIå±‚
â”‚   â”œâ”€â”€ main.py                   # FastAPIä¸»åº”ç”¨  
â”‚   â”œâ”€â”€ enhanced_streaming_rca_service.py  # Multi-Agentåè°ƒå™¨
â”‚   â””â”€â”€ streaming_rca_service.py  # åŸºç¡€æµå¼æœåŠ¡
â”œâ”€â”€ agents/                       # Agentå®ç°
â”‚   â”œâ”€â”€ planner_agent.py         # è§„åˆ’Agent
â”‚   â”œâ”€â”€ knowledge_agent.py       # çŸ¥è¯†Agent
â”‚   â”œâ”€â”€ reasoning_agent.py       # æ¨ç†Agent
â”‚   â””â”€â”€ executor_agent.py        # æ‰§è¡ŒAgent
â”œâ”€â”€ services/                     # æ ¸å¿ƒæœåŠ¡
â”‚   â”œâ”€â”€ improved_rag_service.py  # RAGæœç´¢å¼•æ“
â”‚   â”œâ”€â”€ embedding_service.py     # å‘é‡åŒ–æœåŠ¡
â”‚   â”œâ”€â”€ topology_service.py      # æ‹“æ‰‘åˆ†ææœåŠ¡
â”‚   â””â”€â”€ llm_adapter.py          # LLMé€‚é…å™¨
â”œâ”€â”€ utils/                        # å·¥å…·ç±»
â”‚   â”œâ”€â”€ multi_agent_task_manager.py  # ä»»åŠ¡ç®¡ç†
â”‚   â”œâ”€â”€ ner_extractor.py         # NERæå–å™¨
â”‚   â””â”€â”€ performance_monitor.py    # æ€§èƒ½ç›‘æ§
â””â”€â”€ models/                       # æ•°æ®æ¨¡å‹
    â”œâ”€â”€ task_models.py           # ä»»åŠ¡æ•°æ®æ¨¡å‹
    â””â”€â”€ agent_models.py          # Agentæ•°æ®æ¨¡å‹
```

### æ–°Agentå¼€å‘æŒ‡å—
```python
# æ–°Agentå¼€å‘æ¨¡æ¿
class NewCustomAgent:
    """æ–°Agentå¼€å‘æ¨¡æ¿"""
    
    def __init__(self):
        self.agent_id = "new_custom_agent"
        self.agent_name = "æ–°å®šåˆ¶Agent"
        self.capabilities = ["capability1", "capability2"]
    
    async def initialize(self):
        """Agentåˆå§‹åŒ–"""
        # åˆå§‹åŒ–èµ„æºã€è¿æ¥ç­‰
        pass
    
    async def process(self, task_context: Dict[str, Any]) -> Dict[str, Any]:
        """Agentæ ¸å¿ƒå¤„ç†é€»è¾‘"""
        try:
            # 1. è§£æä»»åŠ¡ä¸Šä¸‹æ–‡
            message = task_context.get("message")
            previous_results = task_context.get("previous_results", [])
            
            # 2. æ‰§è¡ŒAgentç‰¹å®šé€»è¾‘
            result = await self._execute_specific_logic(message, previous_results)
            
            # 3. è¿”å›æ ‡å‡†æ ¼å¼ç»“æœ
            return {
                "success": True,
                "summary": "Agentæ‰§è¡Œæ‘˜è¦",
                "conclusion": "Agentç»“è®º",
                "confidence": 0.85,
                "data": result,
                "agent_id": self.agent_id
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "agent_id": self.agent_id
            }
    
    async def _execute_specific_logic(self, message: str, context: List[Dict]) -> Dict:
        """Agentç‰¹å®šé€»è¾‘å®ç°"""
        # å®ç°å…·ä½“çš„Agenté€»è¾‘
        pass
    
    def get_metadata(self) -> Dict[str, Any]:
        """è¿”å›Agentå…ƒæ•°æ®"""
        return {
            "agent_id": self.agent_id,
            "name": self.agent_name,
            "capabilities": self.capabilities,
            "version": "1.0.0"
        }
```

### æµ‹è¯•ç­–ç•¥
```python
# æµ‹è¯•ç”¨ä¾‹ç¤ºä¾‹
import pytest
from unittest.mock import AsyncMock, MagicMock

class TestMultiAgentSystem:
    """Multi-Agentç³»ç»Ÿæµ‹è¯•"""
    
    @pytest.fixture
    async def mock_services(self):
        """æ¨¡æ‹Ÿä¾èµ–æœåŠ¡"""
        return {
            "rag_service": AsyncMock(),
            "neo4j_service": AsyncMock(), 
            "embedding_service": AsyncMock()
        }
    
    async def test_agent_workflow_success(self, mock_services):
        """æµ‹è¯•Agentåä½œæµç¨‹æˆåŠŸæƒ…å†µ"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        task_id = "test_task_123"
        message = "service-b CPUå¼‚å¸¸"
        
        # æ¨¡æ‹ŸæœåŠ¡å“åº”
        mock_services["rag_service"].hybrid_search.return_value = {
            "results": [{"content": "test evidence", "score": 0.9}]
        }
        
        # æ‰§è¡Œæµ‹è¯•
        service = EnhancedStreamingRCAService()
        task_id = await service.start_multi_agent_analysis(message, "test_user")
        
        # ç­‰å¾…å®Œæˆ
        await asyncio.sleep(2)
        
        # éªŒè¯ç»“æœ
        status = service.get_multi_agent_task_status(task_id)
        assert status["status"] == "completed"
        assert status["final_result"] is not None
        assert len(status["intermediate_conclusions"]) > 0
    
    async def test_agent_workflow_with_interrupt(self, mock_services):
        """æµ‹è¯•ç”¨æˆ·ä¸­æ–­åŠŸèƒ½"""
        # æµ‹è¯•ä¸­æ–­é€»è¾‘
        pass
    
    async def test_error_handling_and_fallback(self, mock_services):
        """æµ‹è¯•é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥"""
        # æµ‹è¯•é”™è¯¯æ¢å¤é€»è¾‘
        pass
```

---

## ğŸ“ˆ æ€§èƒ½åŸºå‡†æµ‹è¯•

### åŸºå‡†æµ‹è¯•ç»“æœ
```yaml
æµ‹è¯•ç¯å¢ƒ:
  CPU: 16æ ¸å¿ƒ 2.4GHz
  å†…å­˜: 32GB RAM  
  å­˜å‚¨: SSD 1TB
  ç½‘ç»œ: 1Gbps

æ€§èƒ½åŸºå‡†:
  RCAä»»åŠ¡å¤„ç†:
    - å¹³å‡å®Œæˆæ—¶é—´: 24.5ç§’
    - 95åˆ†ä½å®Œæˆæ—¶é—´: 35.2ç§’  
    - å¹¶å‘å¤„ç†èƒ½åŠ›: 15 ä»»åŠ¡/åˆ†é’Ÿ
    - æˆåŠŸç‡: 96.8%
  
  RAGæœç´¢æ€§èƒ½:
    - å¹³å‡å“åº”æ—¶é—´: 15ms
    - 95åˆ†ä½å“åº”æ—¶é—´: 45ms
    - ååé‡: 1200 æŸ¥è¯¢/ç§’
    - å‡†ç¡®ç‡: 94.2%
  
  Agentåä½œæ•ˆç‡:
    - Planner Agent: å¹³å‡2.1ç§’
    - Knowledge Agent: å¹³å‡8.7ç§’
    - Reasoning Agent: å¹³å‡5.2ç§’  
    - Executor Agent: å¹³å‡3.8ç§’
    - åˆ‡æ¢å»¶è¿Ÿ: å¹³å‡78ms

èµ„æºä½¿ç”¨:
  - CPUä½¿ç”¨ç‡: å¹³å‡45%, å³°å€¼72%
  - å†…å­˜ä½¿ç”¨: å¹³å‡12GB, å³°å€¼18GB
  - ç£ç›˜I/O: å¹³å‡200MB/sè¯»å–, 50MB/så†™å…¥
  - ç½‘ç»œæµé‡: å¹³å‡30MB/s
```

---

> ğŸ’¡ **å¼€å‘å»ºè®®**:
> - å§‹ç»ˆä½¿ç”¨å¼‚æ­¥ç¼–ç¨‹æ¨¡å¼ï¼Œé¿å…é˜»å¡æ“ä½œ
> - åˆç†ä½¿ç”¨ç¼“å­˜ï¼Œä½†æ³¨æ„ç¼“å­˜ä¸€è‡´æ€§
> - ç›‘æ§å…³é”®æ€§èƒ½æŒ‡æ ‡ï¼ŒåŠæ—¶å‘ç°ç“¶é¢ˆ
> - ç¼–å†™å…¨é¢çš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
> - ä½¿ç”¨ç±»å‹æç¤ºæé«˜ä»£ç å¯ç»´æŠ¤æ€§
> - éµå¾ªSOLIDåŸåˆ™ï¼Œä¿æŒä»£ç çš„å¯æ‰©å±•æ€§