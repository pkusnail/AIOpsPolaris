"""
ç®€åŒ–çš„Pipelineè¿è¡Œè„šæœ¬
é¿å…å¤æ‚çš„å¯¼å…¥ä¾èµ–é—®é¢˜
"""

import asyncio
import sys
import os
import json
import logging
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'src'))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def setup_rag_collections():
    """å»ºç«‹RAG Collections"""
    print("ğŸ”§ å»ºç«‹RAG Collections...")
    try:
        import weaviate
        
        client = weaviate.Client(url="http://localhost:8080")
        
        # åˆ é™¤ç°æœ‰collections
        existing_collections = ["EmbeddingCollection", "FullTextCollection"]
        for collection_name in existing_collections:
            try:
                client.schema.delete_class(collection_name)
                print(f"   åˆ é™¤ç°æœ‰collection: {collection_name}")
            except:
                pass
        
        # åˆ›å»ºEmbeddingCollection
        embedding_schema = {
            "class": "EmbeddingCollection",
            "description": "å­˜å‚¨å‘é‡åµŒå…¥çš„é›†åˆï¼Œæ”¯æŒè¯­ä¹‰æœç´¢",
            "vectorizer": "none",
            "properties": [
                {
                    "name": "content",
                    "dataType": ["text"],
                    "description": "æ–‡æ¡£å†…å®¹"
                },
                {
                    "name": "source_type",
                    "dataType": ["string"],
                    "description": "æ•°æ®æºç±»å‹"
                },
                {
                    "name": "service_name",
                    "dataType": ["string"],
                    "description": "æœåŠ¡åç§°"
                },
                {
                    "name": "hostname",
                    "dataType": ["string"],
                    "description": "ä¸»æœºå"
                },
                {
                    "name": "timestamp",
                    "dataType": ["string"],
                    "description": "æ—¶é—´æˆ³"
                },
                {
                    "name": "log_file",
                    "dataType": ["string"],
                    "description": "æ—¥å¿—æ–‡ä»¶å"
                },
                {
                    "name": "line_number",
                    "dataType": ["int"],
                    "description": "è¡Œå·"
                },
                {
                    "name": "keywords",
                    "dataType": ["string[]"],
                    "description": "å…³é”®è¯"
                }
            ]
        }
        
        client.schema.create_class(embedding_schema)
        print("âœ… EmbeddingCollectionåˆ›å»ºå®Œæˆ")
        
        # åˆ›å»ºFullTextCollection
        fulltext_schema = {
            "class": "FullTextCollection",
            "description": "å­˜å‚¨å…¨æ–‡ç´¢å¼•çš„é›†åˆï¼Œæ”¯æŒBM25æœç´¢",
            "vectorizer": "none",
            "properties": [
                {
                    "name": "content",
                    "dataType": ["text"],
                    "description": "æ–‡æ¡£å†…å®¹",
                    "indexFilterable": True,
                    "indexSearchable": True
                },
                {
                    "name": "source_type",
                    "dataType": ["string"],
                    "description": "æ•°æ®æºç±»å‹",
                    "indexFilterable": True
                },
                {
                    "name": "service_name",
                    "dataType": ["string"],
                    "description": "æœåŠ¡åç§°",
                    "indexFilterable": True
                },
                {
                    "name": "hostname",
                    "dataType": ["string"],
                    "description": "ä¸»æœºå",
                    "indexFilterable": True
                },
                {
                    "name": "timestamp",
                    "dataType": ["string"],
                    "description": "æ—¶é—´æˆ³",
                    "indexFilterable": True
                },
                {
                    "name": "log_file",
                    "dataType": ["string"],
                    "description": "æ—¥å¿—æ–‡ä»¶å",
                    "indexFilterable": True
                },
                {
                    "name": "line_number",
                    "dataType": ["int"],
                    "description": "è¡Œå·",
                    "indexFilterable": True
                },
                {
                    "name": "keywords",
                    "dataType": ["string[]"],
                    "description": "å…³é”®è¯",
                    "indexFilterable": True
                }
            ]
        }
        
        client.schema.create_class(fulltext_schema)
        print("âœ… FullTextCollectionåˆ›å»ºå®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ RAG Collectionsåˆ›å»ºå¤±è´¥: {e}")
        return False


async def process_log_files():
    """å¤„ç†æ—¥å¿—æ–‡ä»¶"""
    print("\nğŸ“‹ å¤„ç†æ—¥å¿—æ–‡ä»¶...")
    try:
        import weaviate
        import re
        from sentence_transformers import SentenceTransformer
        
        client = weaviate.Client(url="http://localhost:8080")
        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        
        logs_dir = Path("./data/logs/")
        if not logs_dir.exists():
            print("âŒ æ—¥å¿—ç›®å½•ä¸å­˜åœ¨")
            return False
        
        # æ—¥å¿—è§£ææ¨¡å¼
        log_pattern = re.compile(
            r'(?P<timestamp>\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d{3}Z)\s+'
            r'\[(?P<level>\w+)\]\s+'
            r'(?P<service>[\w-]+):\s+'
            r'(?P<message>.*)'
        )
        
        processed_count = 0
        
        for log_file in logs_dir.glob("*.log"):
            print(f"   å¤„ç†æ–‡ä»¶: {log_file.name}")
            
            with open(log_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            for line_num, line in enumerate(lines, 1):
                if line.strip() and not line.startswith('#'):
                    match = log_pattern.match(line.strip())
                    
                    if match:
                        groups = match.groupdict()
                        content = f"[{groups['level']}] {groups['service']}: {groups['message']}"
                        
                        # æå–å…³é”®è¯
                        keywords = []
                        if 'CPU' in groups['message']:
                            keywords.append('CPU')
                        if 'error' in groups['message'].lower():
                            keywords.append('error')
                        if 'timeout' in groups['message'].lower():
                            keywords.append('timeout')
                        if groups['service']:
                            keywords.append(groups['service'])
                        
                        # ç”Ÿæˆå‘é‡
                        vector = model.encode(content).tolist()
                        
                        # æ•°æ®å¯¹è±¡
                        data_obj = {
                            "content": content,
                            "source_type": "logs",
                            "service_name": groups['service'],
                            "hostname": "unknown",
                            "timestamp": groups['timestamp'],
                            "log_file": log_file.name,
                            "line_number": line_num,
                            "keywords": keywords
                        }
                        
                        # æ·»åŠ åˆ°EmbeddingCollection
                        client.data_object.create(
                            data_object=data_obj,
                            class_name="EmbeddingCollection",
                            vector=vector
                        )
                        
                        # æ·»åŠ åˆ°FullTextCollection
                        client.data_object.create(
                            data_object=data_obj,
                            class_name="FullTextCollection"
                        )
                        
                        processed_count += 1
        
        print(f"âœ… å¤„ç†å®Œæˆ: {processed_count} æ¡æ—¥å¿—è®°å½•")
        return processed_count > 0
        
    except Exception as e:
        print(f"âŒ æ—¥å¿—æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
        return False


async def process_knowledge_files():
    """å¤„ç†çŸ¥è¯†æ–‡ä»¶"""
    print("\nğŸ“š å¤„ç†çŸ¥è¯†æ–‡ä»¶...")
    try:
        import weaviate
        from sentence_transformers import SentenceTransformer
        
        client = weaviate.Client(url="http://localhost:8080")
        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        
        processed_count = 0
        
        # å¤„ç†Wikiæ•°æ®
        wiki_file = Path("./data/wiki/sample_wiki.json")
        if wiki_file.exists():
            with open(wiki_file, 'r', encoding='utf-8') as f:
                wiki_data = json.load(f)
            
            print(f"   å¤„ç†Wikiæ•°æ®: {len(wiki_data)} ä¸ªæ–‡æ¡£")
            
            for doc in wiki_data:
                content = f"{doc.get('title', '')}: {doc.get('content', '')}"
                
                if len(content) > 50:  # åªå¤„ç†æœ‰æ„ä¹‰çš„å†…å®¹
                    # ç”Ÿæˆå‘é‡
                    vector = model.encode(content).tolist()
                    
                    # æå–å…³é”®è¯
                    keywords = []
                    content_lower = content.lower()
                    if 'service' in content_lower:
                        keywords.append('service')
                    if 'kubernetes' in content_lower:
                        keywords.append('kubernetes')
                    if 'cpu' in content_lower:
                        keywords.append('cpu')
                    if 'memory' in content_lower:
                        keywords.append('memory')
                    
                    data_obj = {
                        "content": content,
                        "source_type": "wiki",
                        "service_name": "documentation",
                        "hostname": "wiki",
                        "timestamp": datetime.now().isoformat() + "Z",
                        "log_file": "wiki",
                        "line_number": 0,
                        "keywords": keywords
                    }
                    
                    # æ·»åŠ åˆ°collections
                    client.data_object.create(
                        data_object=data_obj,
                        class_name="EmbeddingCollection",
                        vector=vector
                    )
                    
                    client.data_object.create(
                        data_object=data_obj,
                        class_name="FullTextCollection"
                    )
                    
                    processed_count += 1
        
        # å¤„ç†GitLabæ•°æ®
        gitlab_file = Path("./data/gitlab/sample_gitlab.json")
        if gitlab_file.exists():
            with open(gitlab_file, 'r', encoding='utf-8') as f:
                gitlab_data = json.load(f)
            
            print(f"   å¤„ç†GitLabæ•°æ®: {len(gitlab_data)} ä¸ªé¡¹ç›®")
            
            for project in gitlab_data:
                content = f"Project: {project.get('name', '')}, Description: {project.get('description', '')}"
                
                if len(content) > 50:
                    vector = model.encode(content).tolist()
                    
                    keywords = ['gitlab', 'project']
                    if project.get('name'):
                        keywords.append(project['name'])
                    
                    data_obj = {
                        "content": content,
                        "source_type": "gitlab",
                        "service_name": project.get('name', 'unknown'),
                        "hostname": "gitlab",
                        "timestamp": datetime.now().isoformat() + "Z",
                        "log_file": "gitlab",
                        "line_number": 0,
                        "keywords": keywords
                    }
                    
                    client.data_object.create(
                        data_object=data_obj,
                        class_name="EmbeddingCollection",
                        vector=vector
                    )
                    
                    client.data_object.create(
                        data_object=data_obj,
                        class_name="FullTextCollection"
                    )
                    
                    processed_count += 1
        
        # å¤„ç†Jiraæ•°æ®
        jira_file = Path("./data/jira/sample_jira.json")
        if jira_file.exists():
            with open(jira_file, 'r', encoding='utf-8') as f:
                jira_data = json.load(f)
            
            print(f"   å¤„ç†Jiraæ•°æ®: {len(jira_data)} ä¸ªå·¥å•")
            
            for issue in jira_data:
                content = f"Issue: {issue.get('summary', '')}, Description: {issue.get('description', '')}"
                
                if len(content) > 50:
                    vector = model.encode(content).tolist()
                    
                    keywords = ['jira', 'issue']
                    if issue.get('summary'):
                        keywords.append('bug' if 'bug' in issue['summary'].lower() else 'task')
                    
                    data_obj = {
                        "content": content,
                        "source_type": "jira",
                        "service_name": issue.get('project', 'unknown'),
                        "hostname": "jira",
                        "timestamp": datetime.now().isoformat() + "Z",
                        "log_file": "jira",
                        "line_number": 0,
                        "keywords": keywords
                    }
                    
                    client.data_object.create(
                        data_object=data_obj,
                        class_name="EmbeddingCollection",
                        vector=vector
                    )
                    
                    client.data_object.create(
                        data_object=data_obj,
                        class_name="FullTextCollection"
                    )
                    
                    processed_count += 1
        
        print(f"âœ… çŸ¥è¯†æ–‡ä»¶å¤„ç†å®Œæˆ: {processed_count} æ¡è®°å½•")
        return processed_count > 0
        
    except Exception as e:
        print(f"âŒ çŸ¥è¯†æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
        return False


async def setup_knowledge_graph():
    """å»ºç«‹çŸ¥è¯†å›¾è°±"""
    print("\nğŸ•¸ï¸ å»ºç«‹çŸ¥è¯†å›¾è°±...")
    try:
        from neo4j import GraphDatabase
        
        driver = GraphDatabase.driver(
            "bolt://localhost:7687",
            auth=("neo4j", "aiops123")
        )
        
        # åˆ›å»ºåŸºç¡€å®ä½“
        with driver.session() as session:
            # åˆ›å»ºæœåŠ¡èŠ‚ç‚¹
            services = ["service-a", "service-b", "service-c", "database", "redis"]
            
            for service in services:
                session.run(
                    "MERGE (s:Service {name: $name})",
                    name=service
                )
            
            # åˆ›å»ºä¸»æœºèŠ‚ç‚¹
            hosts = ["host-1", "host-2", "d1"]
            
            for host in hosts:
                session.run(
                    "MERGE (h:Host {name: $name})",
                    name=host
                )
            
            # åˆ›å»ºé—®é¢˜èŠ‚ç‚¹
            issues = [
                {"name": "CPU overload", "type": "performance", "service": "service-b"},
                {"name": "Memory leak", "type": "memory", "service": "service-a"},
                {"name": "Disk IO bottleneck", "type": "disk", "service": "database"}
            ]
            
            for issue in issues:
                session.run(
                    """
                    MERGE (i:Issue {name: $name, type: $type})
                    MERGE (s:Service {name: $service})
                    MERGE (i)-[:AFFECTS]->(s)
                    """,
                    name=issue["name"],
                    type=issue["type"],
                    service=issue["service"]
                )
            
            # åˆ›å»ºä¾èµ–å…³ç³»
            dependencies = [
                ("service-a", "database"),
                ("service-b", "database"),
                ("service-b", "redis"),
                ("service-c", "service-a")
            ]
            
            for from_service, to_service in dependencies:
                session.run(
                    """
                    MERGE (from:Service {name: $from_service})
                    MERGE (to:Service {name: $to_service})
                    MERGE (from)-[:DEPENDS_ON]->(to)
                    """,
                    from_service=from_service,
                    to_service=to_service
                )
            
            # åˆ›å»ºéƒ¨ç½²å…³ç³»
            deployments = [
                ("service-a", "host-1"),
                ("service-b", "host-2"),
                ("database", "d1"),
                ("redis", "host-1")
            ]
            
            for service, host in deployments:
                session.run(
                    """
                    MERGE (s:Service {name: $service})
                    MERGE (h:Host {name: $host})
                    MERGE (s)-[:DEPLOYED_ON]->(h)
                    """,
                    service=service,
                    host=host
                )
        
        # ç»Ÿè®¡åˆ›å»ºçš„èŠ‚ç‚¹å’Œå…³ç³»
        with driver.session() as session:
            node_count = session.run("MATCH (n) RETURN count(n) as count").single()["count"]
            rel_count = session.run("MATCH ()-[r]->() RETURN count(r) as count").single()["count"]
            
            print(f"âœ… çŸ¥è¯†å›¾è°±åˆ›å»ºå®Œæˆ:")
            print(f"   èŠ‚ç‚¹æ•°: {node_count}")
            print(f"   å…³ç³»æ•°: {rel_count}")
        
        driver.close()
        return True
        
    except Exception as e:
        print(f"âŒ çŸ¥è¯†å›¾è°±åˆ›å»ºå¤±è´¥: {e}")
        return False


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è¿è¡ŒRAG Pipelines")
    print("=" * 50)
    
    success_count = 0
    total_steps = 4
    
    # 1. å»ºç«‹RAG Collections
    if await setup_rag_collections():
        success_count += 1
    
    # 2. å¤„ç†æ—¥å¿—æ–‡ä»¶
    if await process_log_files():
        success_count += 1
    
    # 3. å¤„ç†çŸ¥è¯†æ–‡ä»¶
    if await process_knowledge_files():
        success_count += 1
    
    # 4. å»ºç«‹çŸ¥è¯†å›¾è°±
    if await setup_knowledge_graph():
        success_count += 1
    
    print("\n" + "=" * 50)
    print(f"ğŸ“Š Pipelineè¿è¡Œç»“æœ: {success_count}/{total_steps} æ­¥éª¤æˆåŠŸ")
    
    if success_count == total_steps:
        print("ğŸ‰ æ‰€æœ‰Pipelinesè¿è¡ŒæˆåŠŸ!")
        print("ğŸ’¡ ç°åœ¨å¯ä»¥è¿è¡Œagentæµ‹è¯•éªŒè¯RAGåŠŸèƒ½")
    elif success_count >= total_steps * 0.7:
        print("âœ… å¤§éƒ¨åˆ†æ­¥éª¤æˆåŠŸ")
        print("ğŸ’¡ å»ºè®®æ£€æŸ¥å¤±è´¥çš„æ­¥éª¤")
    else:
        print("âš ï¸ å¤šä¸ªæ­¥éª¤å¤±è´¥")
        print("ğŸ’¡ å»ºè®®æ£€æŸ¥æœåŠ¡çŠ¶æ€å’Œæ•°æ®å®Œæ•´æ€§")


if __name__ == "__main__":
    asyncio.run(main())